{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":50464,"databundleVersionId":5332750,"sourceType":"competition"}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import os\n# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:49:28.445449Z","iopub.execute_input":"2025-01-31T13:49:28.445689Z","iopub.status.idle":"2025-01-31T13:49:28.452565Z","shell.execute_reply.started":"2025-01-31T13:49:28.445663Z","shell.execute_reply":"2025-01-31T13:49:28.451058Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport torch.nn.functional as F\n\nfrom transformers import BertTokenizer, BertModel\n\nimport cv2\nfrom PIL import Image\n\nimport os\nfrom tqdm import tqdm\n\nimport re\nfrom io import StringIO\n\nimport time\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:49:28.453425Z","iopub.execute_input":"2025-01-31T13:49:28.453648Z","iopub.status.idle":"2025-01-31T13:50:34.184636Z","shell.execute_reply.started":"2025-01-31T13:49:28.453625Z","shell.execute_reply":"2025-01-31T13:50:34.183417Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1738331421.392876      10 common_lib.cc:612] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:230\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"**Paramètres**","metadata":{}},{"cell_type":"code","source":"SEED = 42\nBATCH_SIZE = 64 \nNUM_EPOCHS = 11\nLEARNING_RATE = 3e-4 #0.0003\nIMAGE_SIZE = 256\nMAX_TEXT_LENGTH = 128\nNUM_LABELS = 19  \n\nTRAIN_CSV_PATH = '/kaggle/input/multi-label-classification-competition-2023/COMP5329S1A2Dataset/train.csv'\nTEST_CSV_PATH = '/kaggle/input/multi-label-classification-competition-2023/COMP5329S1A2Dataset/test.csv'\nIMAGES_DIR = '/kaggle/input/multi-label-classification-competition-2023/COMP5329S1A2Dataset/data'\n\n# Configuration du device (gpu p100)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:34.186213Z","iopub.execute_input":"2025-01-31T13:50:34.186760Z","iopub.status.idle":"2025-01-31T13:50:34.194851Z","shell.execute_reply.started":"2025-01-31T13:50:34.186711Z","shell.execute_reply":"2025-01-31T13:50:34.193573Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# **Chargement et prétraitement des data**","metadata":{}},{"cell_type":"code","source":"def load_data(csv_path):\n    with open(csv_path, 'r') as file:\n        lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n    \n    df = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n    \n    if 'Labels' in df.columns:\n        df['Labels'] = df['Labels'].fillna('')\n    \n    return df\n\ntrain_df = load_data(TRAIN_CSV_PATH)\ntest_df = load_data(TEST_CSV_PATH)\n\nprint(f\"Train data shape: {train_df.shape}\")\nprint(f\"Test data shape: {test_df.shape}\")\n\nprint(\"\\nTrain DataFrame columns:\")\nprint(train_df.columns)\nprint(\"\\nTest DataFrame columns:\")\nprint(test_df.columns)\n\nprint(\"\\nTrain DataFrame - First 5 rows:\")\nprint(train_df.head())\n\nprint(\"\\nTest DataFrame - First 5 rows:\")\nprint(test_df.head())\n\nprint(\"\\nMissing values in Train DataFrame:\")\nprint(train_df.isnull().sum())\nprint(\"\\nMissing values in Test DataFrame:\")\nprint(test_df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:34.195773Z","iopub.execute_input":"2025-01-31T13:50:34.196020Z","iopub.status.idle":"2025-01-31T13:50:34.477680Z","shell.execute_reply.started":"2025-01-31T13:50:34.195998Z","shell.execute_reply":"2025-01-31T13:50:34.476057Z"}},"outputs":[{"name":"stdout","text":"Train data shape: (29996, 3)\nTest data shape: (10000, 2)\n\nTrain DataFrame columns:\nIndex(['ImageID', 'Labels', 'Caption'], dtype='object')\n\nTest DataFrame columns:\nIndex(['ImageID', 'Caption'], dtype='object')\n\nTrain DataFrame - First 5 rows:\n  ImageID  Labels                                            Caption\n0   0.jpg       1   Woman in swim suit holding parasol on sunny day.\n1   1.jpg    1 19  A couple of men riding horses on top of a gree...\n2   2.jpg       1  They are brave for riding in the jungle on tho...\n3   3.jpg  8 3 13  a black and silver clock tower at an intersect...\n4   4.jpg   8 3 7   A train coming to a stop on the tracks out side.\n\nTest DataFrame - First 5 rows:\n     ImageID                                            Caption\n0  30000.jpg  A little girl waring a krispy kreme hat holdin...\n1  30001.jpg  A beautiful young woman holding an orange fris...\n2  30002.jpg  A group of people sitting on couch next to a c...\n3  30003.jpg         A person on a snowboard rides on the hill.\n4  30004.jpg  A man riding a skateboard with a helmet on in ...\n\nMissing values in Train DataFrame:\nImageID    0\nLabels     0\nCaption    0\ndtype: int64\n\nMissing values in Test DataFrame:\nImageID    0\nCaption    0\ndtype: int64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"image_dir = '/kaggle/input/multi-label-classification-competition-2023/COMP5329S1A2Dataset/data'\nimage_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\nprint(f\"Nombre d'images dans le dossier : {len(image_files)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:34.478759Z","iopub.execute_input":"2025-01-31T13:50:34.479245Z","iopub.status.idle":"2025-01-31T13:50:35.196047Z","shell.execute_reply.started":"2025-01-31T13:50:34.479212Z","shell.execute_reply":"2025-01-31T13:50:35.194527Z"}},"outputs":[{"name":"stdout","text":"Nombre d'images dans le dossier : 40000\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# **Transformation d'image et tokenizer**","metadata":{}},{"cell_type":"code","source":"image_transforms = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Fonction pour encoder les labels\ndef encode_labels(labels_str):\n    labels = [int(label) for label in labels_str.split() if label != '12']\n    encoded = torch.zeros(NUM_LABELS)\n    for label in labels:\n        if 1 <= label <= NUM_LABELS:\n            encoded[label-1] = 1  \n    return encoded\n\n# Fonction pour vérifier la distribution des labels (à garder pour référence future)\ndef check_label_distribution(dataframe):\n    all_labels = dataframe['Labels'].str.split().explode()\n    label_counts = all_labels.value_counts().sort_index()\n    print(\"Label distribution:\")\n    print(label_counts)\n    print(f\"\\nUnique labels: {sorted(label_counts.index.astype(int))}\")\n    print(f\"Min label: {label_counts.index.astype(int).min()}\")\n    print(f\"Max label: {label_counts.index.astype(int).max()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:35.197453Z","iopub.execute_input":"2025-01-31T13:50:35.197804Z","iopub.status.idle":"2025-01-31T13:50:35.953954Z","shell.execute_reply.started":"2025-01-31T13:50:35.197766Z","shell.execute_reply":"2025-01-31T13:50:35.952536Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# image_transforms = transforms.Compose([\n#     transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n# ])\n\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# # Fonction pour encoder les labels\n# def encode_labels(labels_str):\n#     labels = [int(label) for label in labels_str.split() if label != '12']\n#     encoded = torch.zeros(NUM_LABELS)\n#     for label in labels:\n#         if 1 <= label <= NUM_LABELS:\n#             encoded[label-1] = 1  \n#     return encoded\n\n# # Fonction pour vérifier la distribution des labels (à garder pour référence future)\n# def check_label_distribution(dataframe):\n#     all_labels = dataframe['Labels'].str.split().explode()\n#     label_counts = all_labels.value_counts().sort_index()\n#     print(\"Label distribution:\")\n#     print(label_counts)\n#     print(f\"\\nUnique labels: {sorted(label_counts.index.astype(int))}\")\n#     print(f\"Min label: {label_counts.index.astype(int).min()}\")\n#     print(f\"Max label: {label_counts.index.astype(int).max()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:35.957721Z","iopub.execute_input":"2025-01-31T13:50:35.957981Z","iopub.status.idle":"2025-01-31T13:50:35.962595Z","shell.execute_reply.started":"2025-01-31T13:50:35.957954Z","shell.execute_reply":"2025-01-31T13:50:35.961241Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"check_label_distribution(train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:35.963444Z","iopub.execute_input":"2025-01-31T13:50:35.963691Z","iopub.status.idle":"2025-01-31T13:50:36.507511Z","shell.execute_reply.started":"2025-01-31T13:50:35.963668Z","shell.execute_reply":"2025-01-31T13:50:36.505988Z"}},"outputs":[{"name":"stdout","text":"Label distribution:\nLabels\n1     22794\n10     1471\n11      604\n13      605\n14      251\n15     1934\n16     1099\n17     1430\n18     1525\n19     1020\n2      1162\n3      4364\n4      1272\n5      1130\n6      1394\n7      1221\n8      2210\n9      1042\nName: count, dtype: int64\n\nUnique labels: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19]\nMin label: 1\nMax label: 19\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# **Dataset Personnalisé**","metadata":{}},{"cell_type":"code","source":"class MultiModalDataset(Dataset):\n    def __init__(self, dataframe, image_dir, transform=None):\n        self.dataframe = dataframe\n        self.image_dir = image_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_path = os.path.join(self.image_dir, row['ImageID'])\n        image = Image.open(image_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        caption = row['Caption']\n        encoded_caption = tokenizer(caption, padding='max_length', max_length=MAX_TEXT_LENGTH, truncation=True, return_tensors='pt')\n        \n        if 'Labels' in row:\n            labels = encode_labels(row['Labels'])\n            return image, encoded_caption, labels\n        else:\n            return image, encoded_caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:36.508365Z","iopub.execute_input":"2025-01-31T13:50:36.508640Z","iopub.status.idle":"2025-01-31T13:50:36.516112Z","shell.execute_reply.started":"2025-01-31T13:50:36.508608Z","shell.execute_reply":"2025-01-31T13:50:36.514780Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Split\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=SEED)\n\n# Dataset\ntrain_dataset = MultiModalDataset(train_df, IMAGES_DIR, transform=image_transforms)\nval_dataset = MultiModalDataset(val_df, IMAGES_DIR, transform=image_transforms)\ntest_dataset = MultiModalDataset(test_df, IMAGES_DIR, transform=image_transforms)\n\n# DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:36.516946Z","iopub.execute_input":"2025-01-31T13:50:36.517177Z","iopub.status.idle":"2025-01-31T13:50:36.536415Z","shell.execute_reply.started":"2025-01-31T13:50:36.517154Z","shell.execute_reply":"2025-01-31T13:50:36.535277Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"print(f\"Training examples: {len(train_df)}\")\nprint(f\"Validation examples: {len(val_df)}\")\nprint(f\"Testing examples: {len(test_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:36.537499Z","iopub.execute_input":"2025-01-31T13:50:36.537782Z","iopub.status.idle":"2025-01-31T13:50:36.542490Z","shell.execute_reply.started":"2025-01-31T13:50:36.537755Z","shell.execute_reply":"2025-01-31T13:50:36.541323Z"}},"outputs":[{"name":"stdout","text":"Training examples: 23996\nValidation examples: 6000\nTesting examples: 10000\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# **Modèle (ResNet & Bert)**","metadata":{}},{"cell_type":"code","source":"from torchvision.models import efficientnet_v2_s\nfrom transformers import AutoTokenizer, AutoModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:36.543357Z","iopub.execute_input":"2025-01-31T13:50:36.543596Z","iopub.status.idle":"2025-01-31T13:50:36.573381Z","shell.execute_reply.started":"2025-01-31T13:50:36.543574Z","shell.execute_reply":"2025-01-31T13:50:36.572339Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# import torch.nn as nn\n# import torchvision.models as models\n\n# class MultimodalModel(nn.Module):\n#     def __init__(self, num_labels):\n#         super(MultimodalModel, self).__init__()\n        \n#         # Vision: EfficientNetV2-S\n#         self.vision_model = models.efficientnet_v2_s(weights='IMAGENET1K_V1')\n#         self.vision_model.classifier = nn.Sequential(\n#             nn.Linear(1280, 512),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),  # Ajout de Dropout\n#             nn.Linear(512, 256),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),  # Ajout de Dropout\n#         )\n        \n#         # Texte: BERT\n#         self.text_model = BertModel.from_pretrained(\"bert-base-uncased\")\n#         self.text_fc = nn.Sequential(\n#             nn.Linear(768, 256),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),  # Ajout de Dropout\n#         )\n        \n#         # Fusion des features des deux modèles\n#         self.fc = nn.Sequential(\n#             nn.Linear(256 + 256, 128),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),  # Ajout de Dropout\n#             nn.Linear(128, num_labels)\n#         )\n    \n#     def forward(self, image, input_ids, attention_mask):\n#         vision_features = self.vision_model(image)\n#         text_features = self.text_model(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n#         text_features = self.text_fc(text_features)\n\n#         combined = torch.cat((vision_features, text_features), dim=1)\n#         output = self.fc(combined)\n        \n#         return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:36.574439Z","iopub.execute_input":"2025-01-31T13:50:36.574640Z","iopub.status.idle":"2025-01-31T13:50:36.579244Z","shell.execute_reply.started":"2025-01-31T13:50:36.574621Z","shell.execute_reply":"2025-01-31T13:50:36.578072Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class MultimodalModel(nn.Module):\n    def __init__(self, num_labels):\n        super().__init__()\n        # Vision (EfficientNetV2-S)\n        self.vision_model = efficientnet_v2_s(weights=\"IMAGENET1K_V1\")\n        self.vision_model.classifier = nn.Identity()  # Features: 1280-dim\n        \n        # Texte (DeBERTa)\n        self.text_model = AutoModel.from_pretrained(\"microsoft/deberta-base\")  # Features: 768-dim\n        \n        # Fusion par CrossAttention\n        self.cross_attention = CrossAttention(dim_image=1280, dim_text=768, embed_dim=512)\n        \n        # Classifieur (Ajustez la dimension d'entrée)\n        self.classifier = nn.Sequential(\n            nn.Linear(512 + 1280, 512),  # 512 (attn) + 1280 (image)\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_labels)\n        )\n        \n    def forward(self, images, input_ids, attention_mask):\n        # Extraction des features\n        image_features = self.vision_model(images)                      # (Batch, 1280)\n        text_outputs = self.text_model(input_ids, attention_mask)       # (Batch, Seq, 768)\n        text_features = text_outputs.last_hidden_state[:, 0, :]         # (Batch, 768)\n        \n        # Fusion avec CrossAttention\n        fused_features = self.cross_attention(image_features, text_features)  # (Batch, 1792)\n        \n        # Classification\n        outputs = self.classifier(fused_features)\n        return outputs\n        \nclass CrossAttention(nn.Module):\n    def __init__(self, dim_image=1280, dim_text=768, embed_dim=512):\n        super().__init__()\n        # Projections linéaires pour aligner les dimensions\n        self.image_proj = nn.Linear(dim_image, embed_dim)\n        self.text_proj = nn.Linear(dim_text, embed_dim)\n        # Attention multi-têtes\n        self.attention = nn.MultiheadAttention(embed_dim, num_heads=8)\n        \n    def forward(self, image_features, text_features):\n        # Projection des features\n        projected_image = self.image_proj(image_features)  # (Batch, Embed)\n        projected_text = self.text_proj(text_features)      # (Batch, Embed)\n        \n        # Formatage pour MultiheadAttention : (Séquence, Batch, Embed)\n        projected_image = projected_image.unsqueeze(0)      # (1, Batch, 512)\n        projected_text = projected_text.unsqueeze(0)        # (1, Batch, 512)\n        \n        # Calcul de l'attention (Query=Image, Key/Value=Text)\n        attn_output, _ = self.attention(projected_image, projected_text, projected_text)\n        attn_output = attn_output.squeeze(0)  # (Batch, 512)\n        \n        # Fusion avec les features image originales\n        fused_features = torch.cat([attn_output, image_features], dim=1)  # (Batch, 512 + 1280)\n        return fused_features\n\nmodel = MultimodalModel(NUM_LABELS).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:36.580313Z","iopub.execute_input":"2025-01-31T13:50:36.580666Z","iopub.status.idle":"2025-01-31T13:50:42.142005Z","shell.execute_reply.started":"2025-01-31T13:50:36.580636Z","shell.execute_reply":"2025-01-31T13:50:42.140828Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_s-dd5fe13b.pth\n100%|██████████| 82.7M/82.7M [00:00<00:00, 186MB/s] \n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# from torchvision.models import efficientnet_v2_s\n# from transformers import AutoTokenizer, AutoModel\n\n# class MultimodalModel(nn.Module):\n#     def __init__(self, num_labels):\n#         super(MultimodalModel, self).__init__()\n        \n#         # Vision: EfficientNetV2-S\n#         self.vision_model = efficientnet_v2_s(weights=\"IMAGENET1K_V1\")\n#         # Get the number of features before the classifier\n#         vision_features = self.vision_model.classifier[1].in_features  \n#         self.vision_model.classifier = nn.Identity()  # Supprimer la dernière couche\n       \n\n#         # Texte: DeBERTa\n#         self.text_model = AutoModel.from_pretrained(\"microsoft/deberta-base\")\n#         text_features = self.text_model.config.hidden_size\n\n#         # Fusion\n#         self.classifier = nn.Sequential(\n#             nn.Linear(vision_features + text_features, 512),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),\n#             nn.Linear(512, num_labels)\n#         )\n\n#     def forward(self, images, input_ids, attention_mask): # Changed the order of arguments\n#         # Texte\n#         text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n#         text_embeddings = text_outputs[:, 0, :]  # CLS token\n\n#         # Images\n#         image_features = self.vision_model(images)\n\n#         # Concatenation\n#         combined_features = torch.cat((text_embeddings, image_features), dim=1)\n        \n#         # Classification\n#         outputs = self.classifier(combined_features)\n#         return outputs\n\n\n# class CrossAttention(nn.Module):\n#     def __init__(self, dim_image, dim_text):\n#         super(CrossAttention, self).__init__()\n#         self.attention = nn.MultiheadAttention(embed_dim=dim_image, num_heads=8)\n\n#     def forward(self, image_features, text_features):\n#         image_features = image_features.unsqueeze(0)\n#         text_features = text_features.unsqueeze(0)\n\n#         attn_output, _ = self.attention(image_features, text_features, text_features)\n\n#         fused_features = torch.cat([attn_output.squeeze(0), image_features.squeeze(0)], dim=1)\n#         return fused_features\n\n# model = MultimodalModel(NUM_LABELS).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:42.143458Z","iopub.execute_input":"2025-01-31T13:50:42.143799Z","iopub.status.idle":"2025-01-31T13:50:42.148981Z","shell.execute_reply.started":"2025-01-31T13:50:42.143772Z","shell.execute_reply":"2025-01-31T13:50:42.147964Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Affichage des informations sur les datasets et dataloaders\nprint(\"TRAINING\")\nprint(\"training examples: \", len(train_dataset))\nprint(\"batch size: \", BATCH_SIZE)\nprint(\"batches available: \", len(train_loader))\nprint()\nprint(\"TESTING\")\nprint(\"validation examples: \", len(val_dataset))\nprint(\"batch size: \", BATCH_SIZE)\nprint(\"batches available: \", len(val_loader))\nprint()\nprint(\"VALIDATION\")\nprint(\"testing examples: \", len(test_dataset))\nprint(\"batch size: \", BATCH_SIZE)\nprint(\"batches available: \", len(test_loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:42.149756Z","iopub.execute_input":"2025-01-31T13:50:42.149986Z","iopub.status.idle":"2025-01-31T13:50:44.028506Z","shell.execute_reply.started":"2025-01-31T13:50:42.149963Z","shell.execute_reply":"2025-01-31T13:50:44.026955Z"}},"outputs":[{"name":"stdout","text":"TRAINING\ntraining examples:  23996\nbatch size:  64\nbatches available:  375\n\nTESTING\nvalidation examples:  6000\nbatch size:  64\nbatches available:  94\n\nVALIDATION\ntesting examples:  10000\nbatch size:  64\nbatches available:  157\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"**Fonction de perte & optimiseur**","metadata":{}},{"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:44.029365Z","iopub.execute_input":"2025-01-31T13:50:44.029619Z","iopub.status.idle":"2025-01-31T13:50:44.180106Z","shell.execute_reply.started":"2025-01-31T13:50:44.029594Z","shell.execute_reply":"2025-01-31T13:50:44.178188Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"**Fonction d'entrainement & évaluation**","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, dataloader, criterion, optimizer, device, verbose=False):\n    model.train()\n    total_loss = 0\n    total_samples = 0\n    \n    for batch_idx, (images, captions, labels) in enumerate(tqdm(dataloader)):\n        images = images.to(device)\n        input_ids = captions['input_ids'].squeeze(1).to(device)\n        attention_mask = captions['attention_mask'].squeeze(1).to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images, input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        batch_size = images.size(0)\n        total_loss += loss.item() * batch_size\n        total_samples += batch_size\n    \n    return total_loss / total_samples\n\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    total_samples = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, captions, labels in tqdm(dataloader):\n            images = images.to(device)\n            input_ids = captions['input_ids'].squeeze(1).to(device)\n            attention_mask = captions['attention_mask'].squeeze(1).to(device)\n            labels = labels.to(device)\n            \n            outputs = model(images, input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            \n            batch_size = images.size(0)\n            total_loss += loss.item() * batch_size\n            total_samples += batch_size\n            \n            preds = (torch.sigmoid(outputs) > 0.5).float()\n            all_preds.append(preds.cpu())\n            all_labels.append(labels.cpu())\n    \n    # Concaténation pour calculer le F1-score\n    all_preds = torch.cat(all_preds, dim=0)\n    all_labels = torch.cat(all_labels, dim=0)\n    f1 = f1_score(all_labels, all_preds, average='samples')\n    \n    return total_loss / total_samples, f1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:44.181299Z","iopub.execute_input":"2025-01-31T13:50:44.181536Z","iopub.status.idle":"2025-01-31T13:50:44.324654Z","shell.execute_reply.started":"2025-01-31T13:50:44.181514Z","shell.execute_reply":"2025-01-31T13:50:44.323019Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# **Boucle d'entrainement**","metadata":{}},{"cell_type":"code","source":"config = {\n    'patience': 5,               \n    'scheduler_patience': 2,     \n    'scheduler_mode': 'max',     \n    'model_path': 'best_model.pth', \n    'checkpoint_path': 'checkpoint.pth'  # Nouveau fichier de checkpoint\n}\n\nbest_f1 = 0\npatience_counter = 0\n\nscheduler = ReduceLROnPlateau(optimizer, mode=config['scheduler_mode'], \n                              patience=config['scheduler_patience'], verbose=True)\n\nfor epoch in range(NUM_EPOCHS):\n    start_time = time.time()\n    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n    val_loss, val_f1 = evaluate(model, val_loader, criterion, device)\n    epoch_duration = time.time() - start_time\n    \n    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Duration: {epoch_duration:.2f}s\")\n    print(f\"  Train Loss: {train_loss:.4f}\")\n    print(f\"  Val Loss:   {val_loss:.4f}\")\n    print(f\"  Val F1:     {val_f1:.4f}\")\n    \n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        # Sauvegarde complète du modèle\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'loss': val_loss\n        }, config['checkpoint_path'])  \n        print(\"  Model and optimizer saved!\")\n        patience_counter = 0  \n    else:\n        patience_counter += 1\n    \n    if patience_counter >= config['patience']:\n        print(\"  Early stopping!\")\n        break\n    \n    scheduler.step(val_f1)\n    \n    print()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:44.325621Z","iopub.execute_input":"2025-01-31T13:50:44.326007Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"100%|██████████| 375/375 [40:51<00:00,  6.54s/it]\n100%|██████████| 94/94 [03:28<00:00,  2.22s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/11 - Duration: 2660.32s\n  Train Loss: 0.1180\n  Val Loss:   0.0863\n  Val F1:     0.8141\n  Model and optimizer saved!\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 375/375 [38:34<00:00,  6.17s/it]\n100%|██████████| 94/94 [02:58<00:00,  1.90s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/11 - Duration: 2493.42s\n  Train Loss: 0.0849\n  Val Loss:   0.0865\n  Val F1:     0.8234\n  Model and optimizer saved!\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 375/375 [41:45<00:00,  6.68s/it]\n100%|██████████| 94/94 [03:18<00:00,  2.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/11 - Duration: 2704.34s\n  Train Loss: 0.0760\n  Val Loss:   0.0886\n  Val F1:     0.8086\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 375/375 [41:58<00:00,  6.72s/it]\n100%|██████████| 94/94 [03:17<00:00,  2.10s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/11 - Duration: 2715.91s\n  Train Loss: 0.0717\n  Val Loss:   0.0905\n  Val F1:     0.8096\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 375/375 [42:40<00:00,  6.83s/it]\n100%|██████████| 94/94 [03:20<00:00,  2.13s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/11 - Duration: 2760.75s\n  Train Loss: 0.0690\n  Val Loss:   0.0979\n  Val F1:     0.7926\n\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 84/375 [09:31<33:16,  6.86s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# import torch\n\n# checkpoint_path = 'checkpoint.pth'\n\n# try:\n#     checkpoint = torch.load(checkpoint_path)\n#     model.load_state_dict(checkpoint['model_state_dict'])\n#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n#     scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n#     start_epoch = checkpoint['epoch'] + 1  # Reprendre à l'époque suivante\n\n#     print(f\"Checkpoint chargé : reprise à l'époque {start_epoch}\")\n    \n# except FileNotFoundError:\n#     print(\"Aucun checkpoint trouvé, l'entraînement recommence à zéro.\")\n#     start_epoch = 0  # Recommencer à zéro si aucun checkpoint\n\n# # Reprendre l'entraînement à partir de start_epoch\n# for epoch in range(start_epoch, NUM_EPOCHS):\n#     start_time = time.time()\n#     train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n#     val_loss, val_f1 = evaluate(model, val_loader, criterion, device)\n#     epoch_duration = time.time() - start_time\n    \n#     print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Duration: {epoch_duration:.2f}s\")\n#     print(f\"  Train Loss: {train_loss:.4f}\")\n#     print(f\"  Val Loss:   {val_loss:.4f}\")\n#     print(f\"  Val F1:     {val_f1:.4f}\")\n    \n#     if val_f1 > best_f1:\n#         best_f1 = val_f1\n#         torch.save({\n#             'epoch': epoch,\n#             'model_state_dict': model.state_dict(),\n#             'optimizer_state_dict': optimizer.state_dict(),\n#             'scheduler_state_dict': scheduler.state_dict(),\n#             'loss': val_loss\n#         }, checkpoint_path)  \n#         print(\"  Model and optimizer saved!\")\n#         patience_counter = 0  \n#     else:\n#         patience_counter += 1\n    \n#     if patience_counter >= config['patience']:\n#         print(\"  Early stopping!\")\n#         break\n    \n#     scheduler.step(val_f1)\n    \n#     print()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nmodel.load_state_dict(torch.load('best_model.pth'))\nmodel = model.to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(\"Fichier trouvé :\", os.path.exists(\"best_model.pth\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Résumé du Meilleur Modèle Trouvé\")\nprint(\"-\" * 40)\nprint(f\"Meilleur F1-Score (Validation): {best_f1:.4f}\")\nprint(\"\\nHyperparamètres utilisés :\")\nprint(f\"  - Nombre d'époques : {NUM_EPOCHS}\")\nprint(f\"  - Patience pour Early Stopping : {config['patience']}\")\nprint(f\"  - Learning Rate initial : {optimizer.defaults['lr']}\")\nprint(f\"  - Scheduler : ReduceLROnPlateau (patience = {config['scheduler_patience']})\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Génération des prédictions**","metadata":{}},{"cell_type":"code","source":"def generate_predictions(model, dataloader, device):\n    model.eval()\n    all_preds = []\n    \n    with torch.no_grad():\n        for images, captions in tqdm(dataloader, desc=\"Generating predictions\"):\n            images = images.to(device)\n            input_ids = captions['input_ids'].squeeze(1).to(device)\n            attention_mask = captions['attention_mask'].squeeze(1).to(device)\n            \n            outputs = model(images, input_ids, attention_mask)\n            preds = (torch.sigmoid(outputs) > 0.5).float()\n            all_preds.append(preds.cpu())\n    \n    return torch.cat(all_preds, dim=0)\n\n# Charger le meilleur modèle\nmodel.load_state_dict(torch.load('best_model.pth'))\nmodel = model.to(device)\npredictions = generate_predictions(model, test_loader, device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Test dataset length:\", len(test_loader.dataset))\nprint(\"Test dataframe length:\", len(test_df))\nprint(test_df['ImageID'].head()) \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print(f\"Number of images: {len(image_ids)}\")\nprint(f\"Number of predictions: {len(predictions)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Création du fichier de soumission**","metadata":{}},{"cell_type":"code","source":"def create_submission(predictions, test_df):\n    submission = pd.DataFrame({'ImageID': test_df['ImageID']})\n    \n    labels = predictions.numpy()\n    submission['Labels'] = [' '.join(str(i+1) for i in range(NUM_LABELS) if label[i] == 1) for label in labels]\n    \n    submission.to_csv('submission.csv', index=False)\n    print(\"Submission file created!\")\n\ncreate_submission(predictions, test_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df = pd.read_csv('/kaggle/working/submission.csv')\nsubmission_df.head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}