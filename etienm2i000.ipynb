{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":50464,"databundleVersionId":5332750,"sourceType":"competition"}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import os\n# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:49:28.445449Z","iopub.execute_input":"2025-01-31T13:49:28.445689Z","iopub.status.idle":"2025-01-31T13:49:28.452565Z","shell.execute_reply.started":"2025-01-31T13:49:28.445663Z","shell.execute_reply":"2025-01-31T13:49:28.451058Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport torch.nn.functional as F\n\nfrom transformers import BertTokenizer, BertModel\n\nimport cv2\nfrom PIL import Image\n\nimport os\nfrom tqdm import tqdm\n\nimport re\nfrom io import StringIO\n\nimport time\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:49:28.453425Z","iopub.execute_input":"2025-01-31T13:49:28.453648Z","iopub.status.idle":"2025-01-31T13:50:34.184636Z","shell.execute_reply.started":"2025-01-31T13:49:28.453625Z","shell.execute_reply":"2025-01-31T13:50:34.183417Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1738331421.392876      10 common_lib.cc:612] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:230\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"**Paramètres**","metadata":{}},{"cell_type":"code","source":"SEED = 42\nBATCH_SIZE = 64 \nNUM_EPOCHS = 11\nLEARNING_RATE = 3e-4 #0.0003\nIMAGE_SIZE = 256\nMAX_TEXT_LENGTH = 128\nNUM_LABELS = 19  \n\nTRAIN_CSV_PATH = '/kaggle/input/multi-label-classification-competition-2023/COMP5329S1A2Dataset/train.csv'\nTEST_CSV_PATH = '/kaggle/input/multi-label-classification-competition-2023/COMP5329S1A2Dataset/test.csv'\nIMAGES_DIR = '/kaggle/input/multi-label-classification-competition-2023/COMP5329S1A2Dataset/data'\n\n# Configuration du device (gpu p100)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:34.186213Z","iopub.execute_input":"2025-01-31T13:50:34.186760Z","iopub.status.idle":"2025-01-31T13:50:34.194851Z","shell.execute_reply.started":"2025-01-31T13:50:34.186711Z","shell.execute_reply":"2025-01-31T13:50:34.193573Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# **Chargement et prétraitement des data**","metadata":{}},{"cell_type":"code","source":"def load_data(csv_path):\n    with open(csv_path, 'r') as file:\n        lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n    \n    df = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n    \n    if 'Labels' in df.columns:\n        df['Labels'] = df['Labels'].fillna('')\n    \n    return df\n\ntrain_df = load_data(TRAIN_CSV_PATH)\ntest_df = load_data(TEST_CSV_PATH)\n\nprint(f\"Train data shape: {train_df.shape}\")\nprint(f\"Test data shape: {test_df.shape}\")\n\nprint(\"\\nTrain DataFrame columns:\")\nprint(train_df.columns)\nprint(\"\\nTest DataFrame columns:\")\nprint(test_df.columns)\n\nprint(\"\\nTrain DataFrame - First 5 rows:\")\nprint(train_df.head())\n\nprint(\"\\nTest DataFrame - First 5 rows:\")\nprint(test_df.head())\n\nprint(\"\\nMissing values in Train DataFrame:\")\nprint(train_df.isnull().sum())\nprint(\"\\nMissing values in Test DataFrame:\")\nprint(test_df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:34.195773Z","iopub.execute_input":"2025-01-31T13:50:34.196020Z","iopub.status.idle":"2025-01-31T13:50:34.477680Z","shell.execute_reply.started":"2025-01-31T13:50:34.195998Z","shell.execute_reply":"2025-01-31T13:50:34.476057Z"}},"outputs":[{"name":"stdout","text":"Train data shape: (29996, 3)\nTest data shape: (10000, 2)\n\nTrain DataFrame columns:\nIndex(['ImageID', 'Labels', 'Caption'], dtype='object')\n\nTest DataFrame columns:\nIndex(['ImageID', 'Caption'], dtype='object')\n\nTrain DataFrame - First 5 rows:\n  ImageID  Labels                                            Caption\n0   0.jpg       1   Woman in swim suit holding parasol on sunny day.\n1   1.jpg    1 19  A couple of men riding horses on top of a gree...\n2   2.jpg       1  They are brave for riding in the jungle on tho...\n3   3.jpg  8 3 13  a black and silver clock tower at an intersect...\n4   4.jpg   8 3 7   A train coming to a stop on the tracks out side.\n\nTest DataFrame - First 5 rows:\n     ImageID                                            Caption\n0  30000.jpg  A little girl waring a krispy kreme hat holdin...\n1  30001.jpg  A beautiful young woman holding an orange fris...\n2  30002.jpg  A group of people sitting on couch next to a c...\n3  30003.jpg         A person on a snowboard rides on the hill.\n4  30004.jpg  A man riding a skateboard with a helmet on in ...\n\nMissing values in Train DataFrame:\nImageID    0\nLabels     0\nCaption    0\ndtype: int64\n\nMissing values in Test DataFrame:\nImageID    0\nCaption    0\ndtype: int64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"image_dir = '/kaggle/input/multi-label-classification-competition-2023/COMP5329S1A2Dataset/data'\nimage_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\nprint(f\"Nombre d'images dans le dossier : {len(image_files)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:34.478759Z","iopub.execute_input":"2025-01-31T13:50:34.479245Z","iopub.status.idle":"2025-01-31T13:50:35.196047Z","shell.execute_reply.started":"2025-01-31T13:50:34.479212Z","shell.execute_reply":"2025-01-31T13:50:35.194527Z"}},"outputs":[{"name":"stdout","text":"Nombre d'images dans le dossier : 40000\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# **Transformation d'image et tokenizer**","metadata":{}},{"cell_type":"code","source":"image_transforms = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Fonction pour encoder les labels\ndef encode_labels(labels_str):\n    labels = [int(label) for label in labels_str.split() if label != '12']\n    encoded = torch.zeros(NUM_LABELS)\n    for label in labels:\n        if 1 <= label <= NUM_LABELS:\n            encoded[label-1] = 1  \n    return encoded\n\n# Fonction pour vérifier la distribution des labels (à garder pour référence future)\ndef check_label_distribution(dataframe):\n    all_labels = dataframe['Labels'].str.split().explode()\n    label_counts = all_labels.value_counts().sort_index()\n    print(\"Label distribution:\")\n    print(label_counts)\n    print(f\"\\nUnique labels: {sorted(label_counts.index.astype(int))}\")\n    print(f\"Min label: {label_counts.index.astype(int).min()}\")\n    print(f\"Max label: {label_counts.index.astype(int).max()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:35.197453Z","iopub.execute_input":"2025-01-31T13:50:35.197804Z","iopub.status.idle":"2025-01-31T13:50:35.953954Z","shell.execute_reply.started":"2025-01-31T13:50:35.197766Z","shell.execute_reply":"2025-01-31T13:50:35.952536Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# image_transforms = transforms.Compose([\n#     transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n# ])\n\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# # Fonction pour encoder les labels\n# def encode_labels(labels_str):\n#     labels = [int(label) for label in labels_str.split() if label != '12']\n#     encoded = torch.zeros(NUM_LABELS)\n#     for label in labels:\n#         if 1 <= label <= NUM_LABELS:\n#             encoded[label-1] = 1  \n#     return encoded\n\n# # Fonction pour vérifier la distribution des labels (à garder pour référence future)\n# def check_label_distribution(dataframe):\n#     all_labels = dataframe['Labels'].str.split().explode()\n#     label_counts = all_labels.value_counts().sort_index()\n#     print(\"Label distribution:\")\n#     print(label_counts)\n#     print(f\"\\nUnique labels: {sorted(label_counts.index.astype(int))}\")\n#     print(f\"Min label: {label_counts.index.astype(int).min()}\")\n#     print(f\"Max label: {label_counts.index.astype(int).max()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:35.957721Z","iopub.execute_input":"2025-01-31T13:50:35.957981Z","iopub.status.idle":"2025-01-31T13:50:35.962595Z","shell.execute_reply.started":"2025-01-31T13:50:35.957954Z","shell.execute_reply":"2025-01-31T13:50:35.961241Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"check_label_distribution(train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:35.963444Z","iopub.execute_input":"2025-01-31T13:50:35.963691Z","iopub.status.idle":"2025-01-31T13:50:36.507511Z","shell.execute_reply.started":"2025-01-31T13:50:35.963668Z","shell.execute_reply":"2025-01-31T13:50:36.505988Z"}},"outputs":[{"name":"stdout","text":"Label distribution:\nLabels\n1     22794\n10     1471\n11      604\n13      605\n14      251\n15     1934\n16     1099\n17     1430\n18     1525\n19     1020\n2      1162\n3      4364\n4      1272\n5      1130\n6      1394\n7      1221\n8      2210\n9      1042\nName: count, dtype: int64\n\nUnique labels: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19]\nMin label: 1\nMax label: 19\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# **Dataset Personnalisé**","metadata":{}},{"cell_type":"code","source":"class MultiModalDataset(Dataset):\n    def __init__(self, dataframe, image_dir, transform=None):\n        self.dataframe = dataframe\n        self.image_dir = image_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_path = os.path.join(self.image_dir, row['ImageID'])\n        image = Image.open(image_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        caption = row['Caption']\n        encoded_caption = tokenizer(caption, padding='max_length', max_length=MAX_TEXT_LENGTH, truncation=True, return_tensors='pt')\n        \n        if 'Labels' in row:\n            labels = encode_labels(row['Labels'])\n            return image, encoded_caption, labels\n        else:\n            return image, encoded_caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:36.508365Z","iopub.execute_input":"2025-01-31T13:50:36.508640Z","iopub.status.idle":"2025-01-31T13:50:36.516112Z","shell.execute_reply.started":"2025-01-31T13:50:36.508608Z","shell.execute_reply":"2025-01-31T13:50:36.514780Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Split\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=SEED)\n\n# Dataset\ntrain_dataset = MultiModalDataset(train_df, IMAGES_DIR, transform=image_transforms)\nval_dataset = MultiModalDataset(val_df, IMAGES_DIR, transform=image_transforms)\ntest_dataset = MultiModalDataset(test_df, IMAGES_DIR, transform=image_transforms)\n\n# DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:36.516946Z","iopub.execute_input":"2025-01-31T13:50:36.517177Z","iopub.status.idle":"2025-01-31T13:50:36.536415Z","shell.execute_reply.started":"2025-01-31T13:50:36.517154Z","shell.execute_reply":"2025-01-31T13:50:36.535277Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"print(f\"Training examples: {len(train_df)}\")\nprint(f\"Validation examples: {len(val_df)}\")\nprint(f\"Testing examples: {len(test_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:36.537499Z","iopub.execute_input":"2025-01-31T13:50:36.537782Z","iopub.status.idle":"2025-01-31T13:50:36.542490Z","shell.execute_reply.started":"2025-01-31T13:50:36.537755Z","shell.execute_reply":"2025-01-31T13:50:36.541323Z"}},"outputs":[{"name":"stdout","text":"Training examples: 23996\nValidation examples: 6000\nTesting examples: 10000\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# **Modèle (ResNet & Bert)**","metadata":{}},{"cell_type":"code","source":"from torchvision.models import efficientnet_v2_s\nfrom transformers import AutoTokenizer, AutoModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:36.543357Z","iopub.execute_input":"2025-01-31T13:50:36.543596Z","iopub.status.idle":"2025-01-31T13:50:36.573381Z","shell.execute_reply.started":"2025-01-31T13:50:36.543574Z","shell.execute_reply":"2025-01-31T13:50:36.572339Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# import torch.nn as nn\n# import torchvision.models as models\n\n# class MultimodalModel(nn.Module):\n#     def __init__(self, num_labels):\n#         super(MultimodalModel, self).__init__()\n        \n#         # Vision: EfficientNetV2-S\n#         self.vision_model = models.efficientnet_v2_s(weights='IMAGENET1K_V1')\n#         self.vision_model.classifier = nn.Sequential(\n#             nn.Linear(1280, 512),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),  # Ajout de Dropout\n#             nn.Linear(512, 256),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),  # Ajout de Dropout\n#         )\n        \n#         # Texte: BERT\n#         self.text_model = BertModel.from_pretrained(\"bert-base-uncased\")\n#         self.text_fc = nn.Sequential(\n#             nn.Linear(768, 256),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),  # Ajout de Dropout\n#         )\n        \n#         # Fusion des features des deux modèles\n#         self.fc = nn.Sequential(\n#             nn.Linear(256 + 256, 128),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),  # Ajout de Dropout\n#             nn.Linear(128, num_labels)\n#         )\n    \n#     def forward(self, image, input_ids, attention_mask):\n#         vision_features = self.vision_model(image)\n#         text_features = self.text_model(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n#         text_features = self.text_fc(text_features)\n\n#         combined = torch.cat((vision_features, text_features), dim=1)\n#         output = self.fc(combined)\n        \n#         return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:36.574439Z","iopub.execute_input":"2025-01-31T13:50:36.574640Z","iopub.status.idle":"2025-01-31T13:50:36.579244Z","shell.execute_reply.started":"2025-01-31T13:50:36.574621Z","shell.execute_reply":"2025-01-31T13:50:36.578072Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class MultimodalModel(nn.Module):\n    def __init__(self, num_labels):\n        super().__init__()\n        # Vision (EfficientNetV2-S)\n        self.vision_model = efficientnet_v2_s(weights=\"IMAGENET1K_V1\")\n        self.vision_model.classifier = nn.Identity()  # Features: 1280-dim\n        \n        # Texte (DeBERTa)\n        self.text_model = AutoModel.from_pretrained(\"microsoft/deberta-base\")  # Features: 768-dim\n        \n        # Fusion par CrossAttention\n        self.cross_attention = CrossAttention(dim_image=1280, dim_text=768, embed_dim=512)\n        \n        # Classifieur (Ajustez la dimension d'entrée)\n        self.classifier = nn.Sequential(\n            nn.Linear(512 + 1280, 512),  # 512 (attn) + 1280 (image)\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_labels)\n        )\n        \n    def forward(self, images, input_ids, attention_mask):\n        # Extraction des features\n        image_features = self.vision_model(images)                      # (Batch, 1280)\n        text_outputs = self.text_model(input_ids, attention_mask)       # (Batch, Seq, 768)\n        text_features = text_outputs.last_hidden_state[:, 0, :]         # (Batch, 768)\n        \n        # Fusion avec CrossAttention\n        fused_features = self.cross_attention(image_features, text_features)  # (Batch, 1792)\n        \n        # Classification\n        outputs = self.classifier(fused_features)\n        return outputs\n        \nclass CrossAttention(nn.Module):\n    def __init__(self, dim_image=1280, dim_text=768, embed_dim=512):\n        super().__init__()\n        # Projections linéaires pour aligner les dimensions\n        self.image_proj = nn.Linear(dim_image, embed_dim)\n        self.text_proj = nn.Linear(dim_text, embed_dim)\n        # Attention multi-têtes\n        self.attention = nn.MultiheadAttention(embed_dim, num_heads=8)\n        \n    def forward(self, image_features, text_features):\n        # Projection des features\n        projected_image = self.image_proj(image_features)  # (Batch, Embed)\n        projected_text = self.text_proj(text_features)      # (Batch, Embed)\n        \n        # Formatage pour MultiheadAttention : (Séquence, Batch, Embed)\n        projected_image = projected_image.unsqueeze(0)      # (1, Batch, 512)\n        projected_text = projected_text.unsqueeze(0)        # (1, Batch, 512)\n        \n        # Calcul de l'attention (Query=Image, Key/Value=Text)\n        attn_output, _ = self.attention(projected_image, projected_text, projected_text)\n        attn_output = attn_output.squeeze(0)  # (Batch, 512)\n        \n        # Fusion avec les features image originales\n        fused_features = torch.cat([attn_output, image_features], dim=1)  # (Batch, 512 + 1280)\n        return fused_features\n\nmodel = MultimodalModel(NUM_LABELS).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:36.580313Z","iopub.execute_input":"2025-01-31T13:50:36.580666Z","iopub.status.idle":"2025-01-31T13:50:42.142005Z","shell.execute_reply.started":"2025-01-31T13:50:36.580636Z","shell.execute_reply":"2025-01-31T13:50:42.140828Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_s-dd5fe13b.pth\n100%|██████████| 82.7M/82.7M [00:00<00:00, 186MB/s] \n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# from torchvision.models import efficientnet_v2_s\n# from transformers import AutoTokenizer, AutoModel\n\n# class MultimodalModel(nn.Module):\n#     def __init__(self, num_labels):\n#         super(MultimodalModel, self).__init__()\n        \n#         # Vision: EfficientNetV2-S\n#         self.vision_model = efficientnet_v2_s(weights=\"IMAGENET1K_V1\")\n#         # Get the number of features before the classifier\n#         vision_features = self.vision_model.classifier[1].in_features  \n#         self.vision_model.classifier = nn.Identity()  # Supprimer la dernière couche\n       \n\n#         # Texte: DeBERTa\n#         self.text_model = AutoModel.from_pretrained(\"microsoft/deberta-base\")\n#         text_features = self.text_model.config.hidden_size\n\n#         # Fusion\n#         self.classifier = nn.Sequential(\n#             nn.Linear(vision_features + text_features, 512),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),\n#             nn.Linear(512, num_labels)\n#         )\n\n#     def forward(self, images, input_ids, attention_mask): # Changed the order of arguments\n#         # Texte\n#         text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n#         text_embeddings = text_outputs[:, 0, :]  # CLS token\n\n#         # Images\n#         image_features = self.vision_model(images)\n\n#         # Concatenation\n#         combined_features = torch.cat((text_embeddings, image_features), dim=1)\n        \n#         # Classification\n#         outputs = self.classifier(combined_features)\n#         return outputs\n\n\n# class CrossAttention(nn.Module):\n#     def __init__(self, dim_image, dim_text):\n#         super(CrossAttention, self).__init__()\n#         self.attention = nn.MultiheadAttention(embed_dim=dim_image, num_heads=8)\n\n#     def forward(self, image_features, text_features):\n#         image_features = image_features.unsqueeze(0)\n#         text_features = text_features.unsqueeze(0)\n\n#         attn_output, _ = self.attention(image_features, text_features, text_features)\n\n#         fused_features = torch.cat([attn_output.squeeze(0), image_features.squeeze(0)], dim=1)\n#         return fused_features\n\n# model = MultimodalModel(NUM_LABELS).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:42.143458Z","iopub.execute_input":"2025-01-31T13:50:42.143799Z","iopub.status.idle":"2025-01-31T13:50:42.148981Z","shell.execute_reply.started":"2025-01-31T13:50:42.143772Z","shell.execute_reply":"2025-01-31T13:50:42.147964Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Affichage des informations sur les datasets et dataloaders\nprint(\"TRAINING\")\nprint(\"training examples: \", len(train_dataset))\nprint(\"batch size: \", BATCH_SIZE)\nprint(\"batches available: \", len(train_loader))\nprint()\nprint(\"TESTING\")\nprint(\"validation examples: \", len(val_dataset))\nprint(\"batch size: \", BATCH_SIZE)\nprint(\"batches available: \", len(val_loader))\nprint()\nprint(\"VALIDATION\")\nprint(\"testing examples: \", len(test_dataset))\nprint(\"batch size: \", BATCH_SIZE)\nprint(\"batches available: \", len(test_loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:42.149756Z","iopub.execute_input":"2025-01-31T13:50:42.149986Z","iopub.status.idle":"2025-01-31T13:50:44.028506Z","shell.execute_reply.started":"2025-01-31T13:50:42.149963Z","shell.execute_reply":"2025-01-31T13:50:44.026955Z"}},"outputs":[{"name":"stdout","text":"TRAINING\ntraining examples:  23996\nbatch size:  64\nbatches available:  375\n\nTESTING\nvalidation examples:  6000\nbatch size:  64\nbatches available:  94\n\nVALIDATION\ntesting examples:  10000\nbatch size:  64\nbatches available:  157\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"**Fonction de perte & optimiseur**","metadata":{}},{"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:44.029365Z","iopub.execute_input":"2025-01-31T13:50:44.029619Z","iopub.status.idle":"2025-01-31T13:50:44.180106Z","shell.execute_reply.started":"2025-01-31T13:50:44.029594Z","shell.execute_reply":"2025-01-31T13:50:44.178188Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"**Fonction d'entrainement & évaluation**","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, dataloader, criterion, optimizer, device, verbose=False):\n    model.train()\n    total_loss = 0\n    total_samples = 0\n    \n    for batch_idx, (images, captions, labels) in enumerate(tqdm(dataloader)):\n        images = images.to(device)\n        input_ids = captions['input_ids'].squeeze(1).to(device)\n        attention_mask = captions['attention_mask'].squeeze(1).to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images, input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        batch_size = images.size(0)\n        total_loss += loss.item() * batch_size\n        total_samples += batch_size\n    \n    return total_loss / total_samples\n\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    total_samples = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, captions, labels in tqdm(dataloader):\n            images = images.to(device)\n            input_ids = captions['input_ids'].squeeze(1).to(device)\n            attention_mask = captions['attention_mask'].squeeze(1).to(device)\n            labels = labels.to(device)\n            \n            outputs = model(images, input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            \n            batch_size = images.size(0)\n            total_loss += loss.item() * batch_size\n            total_samples += batch_size\n            \n            preds = (torch.sigmoid(outputs) > 0.5).float()\n            all_preds.append(preds.cpu())\n            all_labels.append(labels.cpu())\n    \n    # Concaténation pour calculer le F1-score\n    all_preds = torch.cat(all_preds, dim=0)\n    all_labels = torch.cat(all_labels, dim=0)\n    f1 = f1_score(all_labels, all_preds, average='samples')\n    \n    return total_loss / total_samples, f1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:44.181299Z","iopub.execute_input":"2025-01-31T13:50:44.181536Z","iopub.status.idle":"2025-01-31T13:50:44.324654Z","shell.execute_reply.started":"2025-01-31T13:50:44.181514Z","shell.execute_reply":"2025-01-31T13:50:44.323019Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# **Boucle d'entrainement**","metadata":{}},{"cell_type":"code","source":"config = {\n    'patience': 5,               \n    'scheduler_patience': 2,     \n    'scheduler_mode': 'max',     \n    'model_path': 'best_model.pth', \n    'checkpoint_path': 'checkpoint.pth'  # Nouveau fichier de checkpoint\n}\n\nbest_f1 = 0\npatience_counter = 0\n\nscheduler = ReduceLROnPlateau(optimizer, mode=config['scheduler_mode'], \n                              patience=config['scheduler_patience'], verbose=True)\n\nfor epoch in range(NUM_EPOCHS):\n    start_time = time.time()\n    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n    val_loss, val_f1 = evaluate(model, val_loader, criterion, device)\n    epoch_duration = time.time() - start_time\n    \n    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Duration: {epoch_duration:.2f}s\")\n    print(f\"  Train Loss: {train_loss:.4f}\")\n    print(f\"  Val Loss:   {val_loss:.4f}\")\n    print(f\"  Val F1:     {val_f1:.4f}\")\n    \n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        # Sauvegarde complète du modèle\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'loss': val_loss\n        }, config['checkpoint_path'])  \n        print(\"  Model and optimizer saved!\")\n        patience_counter = 0  \n    else:\n        patience_counter += 1\n    \n    if patience_counter >= config['patience']:\n        print(\"  Early stopping!\")\n        break\n    \n    scheduler.step(val_f1)\n    \n    print()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T13:50:44.325621Z","iopub.execute_input":"2025-01-31T13:50:44.326007Z","iopub.status.idle":"2025-01-31T22:10:52.382711Z","shell.execute_reply.started":"2025-01-31T13:50:44.325981Z","shell.execute_reply":"2025-01-31T22:10:52.381711Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"100%|██████████| 375/375 [40:51<00:00,  6.54s/it]\n100%|██████████| 94/94 [03:28<00:00,  2.22s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/11 - Duration: 2660.32s\n  Train Loss: 0.1180\n  Val Loss:   0.0863\n  Val F1:     0.8141\n  Model and optimizer saved!\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 375/375 [38:34<00:00,  6.17s/it]\n100%|██████████| 94/94 [02:58<00:00,  1.90s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/11 - Duration: 2493.42s\n  Train Loss: 0.0849\n  Val Loss:   0.0865\n  Val F1:     0.8234\n  Model and optimizer saved!\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 375/375 [41:45<00:00,  6.68s/it]\n100%|██████████| 94/94 [03:18<00:00,  2.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/11 - Duration: 2704.34s\n  Train Loss: 0.0760\n  Val Loss:   0.0886\n  Val F1:     0.8086\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 375/375 [41:58<00:00,  6.72s/it]\n100%|██████████| 94/94 [03:17<00:00,  2.10s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/11 - Duration: 2715.91s\n  Train Loss: 0.0717\n  Val Loss:   0.0905\n  Val F1:     0.8096\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 375/375 [42:40<00:00,  6.83s/it]\n100%|██████████| 94/94 [03:20<00:00,  2.13s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/11 - Duration: 2760.75s\n  Train Loss: 0.0690\n  Val Loss:   0.0979\n  Val F1:     0.7926\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 375/375 [42:42<00:00,  6.83s/it]\n100%|██████████| 94/94 [03:23<00:00,  2.16s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/11 - Duration: 2766.41s\n  Train Loss: 0.0483\n  Val Loss:   0.0816\n  Val F1:     0.8448\n  Model and optimizer saved!\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 375/375 [42:45<00:00,  6.84s/it]\n100%|██████████| 94/94 [03:29<00:00,  2.23s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/11 - Duration: 2775.21s\n  Train Loss: 0.0380\n  Val Loss:   0.0860\n  Val F1:     0.8449\n  Model and optimizer saved!\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 375/375 [42:40<00:00,  6.83s/it]\n100%|██████████| 94/94 [03:27<00:00,  2.21s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/11 - Duration: 2768.58s\n  Train Loss: 0.0323\n  Val Loss:   0.0892\n  Val F1:     0.8432\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 375/375 [42:36<00:00,  6.82s/it]\n100%|██████████| 94/94 [03:17<00:00,  2.10s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/11 - Duration: 2754.02s\n  Train Loss: 0.0278\n  Val Loss:   0.0939\n  Val F1:     0.8448\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 375/375 [42:28<00:00,  6.80s/it]\n100%|██████████| 94/94 [03:30<00:00,  2.24s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/11 - Duration: 2758.79s\n  Train Loss: 0.0239\n  Val Loss:   0.1018\n  Val F1:     0.8405\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 375/375 [42:57<00:00,  6.87s/it]\n100%|██████████| 94/94 [03:28<00:00,  2.22s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/11 - Duration: 2785.80s\n  Train Loss: 0.0200\n  Val Loss:   0.1003\n  Val F1:     0.8417\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# import torch\n\n# checkpoint_path = 'checkpoint.pth'\n\n# try:\n#     checkpoint = torch.load(checkpoint_path)\n#     model.load_state_dict(checkpoint['model_state_dict'])\n#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n#     scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n#     start_epoch = checkpoint['epoch'] + 1  # Reprendre à l'époque suivante\n\n#     print(f\"Checkpoint chargé : reprise à l'époque {start_epoch}\")\n    \n# except FileNotFoundError:\n#     print(\"Aucun checkpoint trouvé, l'entraînement recommence à zéro.\")\n#     start_epoch = 0  # Recommencer à zéro si aucun checkpoint\n\n# # Reprendre l'entraînement à partir de start_epoch\n# for epoch in range(start_epoch, NUM_EPOCHS):\n#     start_time = time.time()\n#     train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n#     val_loss, val_f1 = evaluate(model, val_loader, criterion, device)\n#     epoch_duration = time.time() - start_time\n    \n#     print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Duration: {epoch_duration:.2f}s\")\n#     print(f\"  Train Loss: {train_loss:.4f}\")\n#     print(f\"  Val Loss:   {val_loss:.4f}\")\n#     print(f\"  Val F1:     {val_f1:.4f}\")\n    \n#     if val_f1 > best_f1:\n#         best_f1 = val_f1\n#         torch.save({\n#             'epoch': epoch,\n#             'model_state_dict': model.state_dict(),\n#             'optimizer_state_dict': optimizer.state_dict(),\n#             'scheduler_state_dict': scheduler.state_dict(),\n#             'loss': val_loss\n#         }, checkpoint_path)  \n#         print(\"  Model and optimizer saved!\")\n#         patience_counter = 0  \n#     else:\n#         patience_counter += 1\n    \n#     if patience_counter >= config['patience']:\n#         print(\"  Early stopping!\")\n#         break\n    \n#     scheduler.step(val_f1)\n    \n#     print()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T22:10:52.383686Z","iopub.execute_input":"2025-01-31T22:10:52.384004Z","iopub.status.idle":"2025-01-31T22:10:52.390281Z","shell.execute_reply.started":"2025-01-31T22:10:52.383973Z","shell.execute_reply":"2025-01-31T22:10:52.389145Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"checkpoint_path = \"checkpoint.pth\"\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model = model.to(device)\n    print(\"Checkpoint chargé avec succès !\")\nelse:\n    print(\"Aucun checkpoint trouvé, réentraîne le modèle.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T22:40:00.719610Z","iopub.execute_input":"2025-01-31T22:40:00.720076Z","iopub.status.idle":"2025-01-31T22:40:01.834513Z","shell.execute_reply.started":"2025-01-31T22:40:00.720039Z","shell.execute_reply":"2025-01-31T22:40:01.833227Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_10/2255998274.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path)\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint chargé avec succès !\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import os\nprint(\"Fichier trouvé :\", os.path.exists(\"checkpoint.pth\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T22:40:33.467903Z","iopub.execute_input":"2025-01-31T22:40:33.468222Z","iopub.status.idle":"2025-01-31T22:40:33.472080Z","shell.execute_reply.started":"2025-01-31T22:40:33.468195Z","shell.execute_reply":"2025-01-31T22:40:33.471290Z"}},"outputs":[{"name":"stdout","text":"Fichier trouvé : True\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"print(\"Résumé du Meilleur Modèle Trouvé\")\nprint(\"-\" * 40)\nprint(f\"Meilleur F1-Score (Validation): {best_f1:.4f}\")\nprint(\"\\nHyperparamètres utilisés :\")\nprint(f\"  - Nombre d'époques : {NUM_EPOCHS}\")\nprint(f\"  - Patience pour Early Stopping : {config['patience']}\")\nprint(f\"  - Learning Rate initial : {optimizer.defaults['lr']}\")\nprint(f\"  - Scheduler : ReduceLROnPlateau (patience = {config['scheduler_patience']})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T22:40:39.992184Z","iopub.execute_input":"2025-01-31T22:40:39.992491Z","iopub.status.idle":"2025-01-31T22:40:39.997253Z","shell.execute_reply.started":"2025-01-31T22:40:39.992459Z","shell.execute_reply":"2025-01-31T22:40:39.996235Z"}},"outputs":[{"name":"stdout","text":"Résumé du Meilleur Modèle Trouvé\n----------------------------------------\nMeilleur F1-Score (Validation): 0.8449\n\nHyperparamètres utilisés :\n  - Nombre d'époques : 11\n  - Patience pour Early Stopping : 5\n  - Learning Rate initial : 0.0003\n  - Scheduler : ReduceLROnPlateau (patience = 2)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# **Génération des prédictions**","metadata":{}},{"cell_type":"code","source":"def generate_predictions(model, dataloader, device):\n    model.eval()\n    all_preds = []\n    \n    with torch.no_grad():\n        for images, captions in tqdm(dataloader, desc=\"Generating predictions\"):\n            images = images.to(device)\n            input_ids = captions['input_ids'].squeeze(1).to(device)\n            attention_mask = captions['attention_mask'].squeeze(1).to(device)\n            \n            outputs = model(images, input_ids, attention_mask)\n            preds = (torch.sigmoid(outputs) > 0.5).float()\n            all_preds.append(preds.cpu())\n    \n    return torch.cat(all_preds, dim=0)\n\n# Charger le meilleur modèle\nmodel.load_state_dict(torch.load('checkpoint.pth'))\nmodel = model.to(device)\npredictions = generate_predictions(model, test_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T22:42:23.495866Z","iopub.execute_input":"2025-01-31T22:42:23.496218Z","iopub.status.idle":"2025-01-31T22:42:24.375455Z","shell.execute_reply.started":"2025-01-31T22:42:23.496189Z","shell.execute_reply":"2025-01-31T22:42:24.373472Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_10/1538762912.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('checkpoint.pth'))\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(all_preds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Charger le meilleur modèle\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoint.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m predictions \u001b[38;5;241m=\u001b[39m generate_predictions(model, test_loader, device)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MultimodalModel:\n\tMissing key(s) in state_dict: \"vision_model.features.0.0.weight\", \"vision_model.features.0.1.weight\", \"vision_model.features.0.1.bias\", \"vision_model.features.0.1.running_mean\", \"vision_model.features.0.1.running_var\", \"vision_model.features.1.0.block.0.0.weight\", \"vision_model.features.1.0.block.0.1.weight\", \"vision_model.features.1.0.block.0.1.bias\", \"vision_model.features.1.0.block.0.1.running_mean\", \"vision_model.features.1.0.block.0.1.running_var\", \"vision_model.features.1.1.block.0.0.weight\", \"vision_model.features.1.1.block.0.1.weight\", \"vision_model.features.1.1.block.0.1.bias\", \"vision_model.features.1.1.block.0.1.running_mean\", \"vision_model.features.1.1.block.0.1.running_var\", \"vision_model.features.2.0.block.0.0.weight\", \"vision_model.features.2.0.block.0.1.weight\", \"vision_model.features.2.0.block.0.1.bias\", \"vision_model.features.2.0.block.0.1.running_mean\", \"vision_model.features.2.0.block.0.1.running_var\", \"vision_model.features.2.0.block.1.0.weight\", \"vision_model.features.2.0.block.1.1.weight\", \"vision_model.features.2.0.block.1.1.bias\", \"vision_model.features.2.0.block.1.1.running_mean\", \"vision_model.features.2.0.block.1.1.running_var\", \"vision_model.features.2.1.block.0.0.weight\", \"vision_model.features.2.1.block.0.1.weight\", \"vision_model.features.2.1.block.0.1.bias\", \"vision_model.features.2.1.block.0.1.running_mean\", \"vision_model.features.2.1.block.0.1.running_var\", \"vision_model.features.2.1.block.1.0.weight\", \"vision_model.features.2.1.block.1.1.weight\", \"vision_model.features.2.1.block.1.1.bias\", \"vision_model.features.2.1.block.1.1.running_mean\", \"vision_model.features.2.1.block.1.1.running_var\", \"vision_model.features.2.2.block.0.0.weight\", \"vision_model.features.2.2.block.0.1.weight\", \"vision_model.features.2.2.block.0.1.bias\", \"vision_model.features.2.2.block.0.1.running_mean\", \"vision_model.features.2.2.block.0.1.running_var\", \"vision_model.features.2.2.block.1.0.weight\", \"vision_model.features.2.2.block.1.1.weight\", \"vision_model.features.2.2.block.1.1.bias\", \"vision_model.features.2.2.block.1.1.running_mean\", \"vision_model.features.2.2.block.1.1.running_var\", \"vision_model.features.2.3.block.0.0.weight\", \"vision_model.features.2.3.block.0.1.weight\", \"vision_model.features.2.3.block.0.1.bias\", \"vision_model.features.2.3.block.0.1.running_mean\", \"vision_model.features.2.3.block.0.1.running_var\", \"vision_model.features.2.3.block.1.0.weight\", \"vision_model.features.2.3.block.1.1.weight\", \"vision_model.features.2.3.block.1.1.bias\", \"vision_model.features.2.3.block.1.1.running_mean\", \"vision_model.features.2.3.block.1.1.running_var\", \"vision_model.features.3.0.block.0.0.weight\", \"vision_model.features.3.0.block.0.1.weight\", \"vision_model.features.3.0.block.0.1.bias\", \"vision_model.features.3.0.block.0.1.running_mean\", \"vision_model.features.3.0.block.0.1.running_var\", \"vision_model.features.3.0.block.1.0.weight\", \"vision_model.features.3.0.block.1.1.weight\", \"vision_model.features.3.0.block.1.1.bias\", \"vision_model.features.3.0.block.1.1.running_mean\", \"vision_model.features.3.0.block.1.1.running_var\", \"vision_model.features.3.1.block.0.0.weight\", \"vision_model.features.3.1.block.0.1.weight\", \"vision_model.features.3.1.block.0.1.bias\", \"vision_model.features.3.1.block.0.1.running_mean\", \"vision_model.features.3.1.block.0.1.running_var\", \"vision_model.features.3.1.block.1.0.weight\", \"vision_model.features.3.1.block.1.1.weight\", \"vision_model.features.3.1.block.1.1.bias\", \"vision_model.features.3.1.block.1.1.running_mean\", \"vision_model.features.3.1.block.1.1.running_var\", \"vision_model.features.3.2.block.0.0.weight\", \"vision_model.features.3.2.block.0.1.weight\", \"vision_model.features.3.2.block.0.1.bias\", \"vision_model.features.3.2.block.0.1.running_mean\", \"vision_model.features.3.2.block.0.1.running_var\", \"vision_model.features.3.2.block.1.0.weight\", \"vision_model.features.3.2.block.1.1.weight\", \"vision_model.features.3.2.block.1.1.bias\", \"vision_model.features.3.2.block.1.1.running_mean\", \"vision_model.features.3.2.block.1.1.running_var\", \"vision_model.features.3.3.block.0.0.weight\", \"vision_model.features.3.3.block.0.1.weight\", \"vision_model.features.3.3.block.0.1.bias\", \"vision_model.features.3.3.block.0.1.running_mean\", \"vision_model.features.3.3.block.0.1.running_var\", \"vision_model.features.3.3.block.1.0.weight\", \"vision_model.features.3.3.block.1.1.weight\", \"vision_model.features.3.3.block.1.1.bias\", \"vision_model.features.3.3.block.1.1.running_mean\", \"vision_model.features.3.3.block.1.1.running_var\", \"vision_model.features.4.0.block.0.0.weight\", \"vision_model.features.4.0.block.0.1.weight\", \"vision_model.features.4.0.block.0.1.bias\", \"vision_model.features.4.0.block.0.1.running_mean\", \"vision_model.features.4.0.block.0.1.running_var\", \"vision_model.features.4.0.block.1.0.weight\", \"vision_model.features.4.0.block.1.1.weight\", \"vision_model.features.4.0.block.1.1.bias\", \"vision_model.features.4.0.block.1.1.running_mean\", \"vision_model.features.4.0.block.1.1.running_var\", \"vision_model.features.4.0.block.2.fc1.weight\", \"vision_model.features.4.0.block.2.fc1.bias\", \"vision_model.features.4.0.block.2.fc2.weight\", \"vision_model.features.4.0.block.2.fc2.bias\", \"vision_model.features.4.0.block.3.0.weight\", \"vision_model.features.4.0.block.3.1.weight\", \"vision_model.features.4.0.block.3.1.bias\", \"vision_model.features.4.0.block.3.1.running_mean\", \"vision_model.features.4.0.block.3.1.running_var\", \"vision_model.features.4.1.block.0.0.weight\", \"vision_model.features.4.1.block.0.1.weight\", \"vision_model.features.4.1.block.0.1.bias\", \"vision_model.features.4.1.block.0.1.running_mean\", \"vision_model.features.4.1.block.0.1.running_var\", \"vision_model.features.4.1.block.1.0.weight\", \"vision_model.features.4.1.block.1.1.weight\", \"vision_model.features.4.1.block.1.1.bias\", \"vision_model.features.4.1.block.1.1.running_mean\", \"vision_model.features.4.1.block.1.1.running_var\", \"vision_model.features.4.1.block.2.fc1.weight\", \"vision_model.features.4.1.block.2.fc1.bias\", \"vision_model.features.4.1.block.2.fc2.weight\", \"vision_model.features.4.1.block.2.fc2.bias\", \"vision_model.features.4.1.block.3.0.weight\", \"vision_model.features.4.1.block.3.1.weight\", \"vision_model.features.4.1.block.3.1.bias\", \"vision_model.features.4.1.block.3.1.running_mean\", \"vision_model.features.4.1.block.3.1.running_var\", \"vision_model.features.4.2.block.0.0.weight\", \"vision_model.features.4.2.block.0.1.weight\", \"vision_model.features.4.2.block.0.1.bias\", \"vision_model.features.4.2.block.0.1.running_mean\", \"vision_model.features.4.2.block.0.1.running_var\", \"vision_model.features.4.2.block.1.0.weight\", \"vision_model.features.4.2.block.1.1.weight\", \"vision_model.features.4.2.block.1.1.bias\", \"vision_model.features.4.2.block.1.1.running_mean\", \"vision_model.features.4.2.block.1.1.running_var\", \"vision_model.features.4.2.block.2.fc1.weight\", \"vision_model.features.4.2.block.2.fc1.bias\", \"vision_model.features.4.2.block.2.fc2.weight\", \"vision_model.features.4.2.block.2.fc2.bias\", \"vision_model.features.4.2.block.3.0.weight\", \"vision_model.features.4.2.block.3.1.weight\", \"vision_model.features.4.2.block.3.1.bias\", \"vision_model.features.4.2.block.3.1.running_mean\", \"vision_model.features.4.2.block.3.1.running_var\", \"vision_model.features.4.3.block.0.0.weight\", \"vision_model.features.4.3.block.0.1.weight\", \"vision_model.features.4.3.block.0.1.bias\", \"vision_model.features.4.3.block.0.1.running_mean\", \"vision_model.features.4.3.block.0.1.running_var\", \"vision_model.features.4.3.block.1.0.weight\", \"vision_model.features.4.3.block.1.1.weight\", \"vision_model.features.4.3.block.1.1.bias\", \"vision_model.features.4.3.block.1.1.running_mean\", \"vision_model.features.4.3.block.1.1.running_var\", \"vision_model.features.4.3.block.2.fc1.weight\", \"vision_model.features.4.3.block.2.fc1.bias\", \"vision_model.features.4.3.block.2.fc2.weight\", \"vision_model.features.4.3.block.2.fc2.bias\", \"vision_model.features.4.3.block.3.0.weight\", \"vision_model.features.4.3.block.3.1.weight\", \"vision_model.features.4.3.block.3.1.bias\", \"vision_model.features.4.3.block.3.1.running_mean\", \"vision_model.features.4.3.block.3.1.running_var\", \"vision_model.features.4.4.block.0.0.weight\", \"vision_model.features.4.4.block.0.1.weight\", \"vision_model.features.4.4.block.0.1.bias\", \"vision_model.features.4.4.block.0.1.running_mean\", \"vision_model.features.4.4.block.0.1.running_var\", \"vision_model.features.4.4.block.1.0.weight\", \"vision_model.features.4.4.block.1.1.weight\", \"vision_model.features.4.4.block.1.1.bias\", \"vision_model.features.4.4.block.1.1.running_mean\", \"vision_model.features.4.4.block.1.1.running_var\", \"vision_model.features.4.4.block.2.fc1.weight\", \"vision_model.features.4.4.block.2.fc1.bias\", \"vision_model.features.4.4.block.2.fc2.weight\", \"vision_model.features.4.4.block.2.fc2.bias\", \"vision_model.features.4.4.block.3.0.weight\", \"vision_model.features.4.4.block.3.1.weight\", \"vision_model.features.4.4.block.3.1.bias\", \"vision_model.features.4.4.block.3.1.running_mean\", \"vision_model.features.4.4.block.3.1.running_var\", \"vision_model.features.4.5.block.0.0.weight\", \"vision_model.features.4.5.block.0.1.weight\", \"vision_model.features.4.5.block.0.1.bias\", \"vision_model.features.4.5.block.0.1.running_mean\", \"vision_model.features.4.5.block.0.1.running_var\", \"vision_model.features.4.5.block.1.0.weight\", \"vision_model.features.4.5.block.1.1.weight\", \"vision_model.features.4.5.block.1.1.bias\", \"vision_model.features.4.5.block.1.1.running_mean\", \"vision_model.features.4.5.block.1.1.running_var\", \"vision_model.features.4.5.block.2.fc1.weight\", \"vision_model.features.4.5.block.2.fc1.bias\", \"vision_model.features.4.5.block.2.fc2.weight\", \"vision_model.features.4.5.block.2.fc2.bias\", \"vision_model.features.4.5.block.3.0.weight\", \"vision_model.features.4.5.block.3.1.weight\", \"vision_model.features.4.5.block.3.1.bias\", \"vision_model.features.4.5.block.3.1.running_mean\", \"vision_model.features.4.5.block.3.1.running_var\", \"vision_model.features.5.0.block.0.0.weight\", \"vision_model.features.5.0.block.0.1.weight\", \"vision_model.features.5.0.block.0.1.bias\", \"vision_model.features.5.0.block.0.1.running_mean\", \"vision_model.features.5.0.block.0.1.running_var\", \"vision_model.features.5.0.block.1.0.weight\", \"vision_model.features.5.0.block.1.1.weight\", \"vision_model.features.5.0.block.1.1.bias\", \"vision_model.features.5.0.block.1.1.running_mean\", \"vision_model.features.5.0.block.1.1.running_var\", \"vision_model.features.5.0.block.2.fc1.weight\", \"vision_model.features.5.0.block.2.fc1.bias\", \"vision_model.features.5.0.block.2.fc2.weight\", \"vision_model.features.5.0.block.2.fc2.bias\", \"vision_model.features.5.0.block.3.0.weight\", \"vision_model.features.5.0.block.3.1.weight\", \"vision_model.features.5.0.block.3.1.bias\", \"vision_model.features.5.0.block.3.1.running_mean\", \"vision_model.features.5.0.block.3.1.running_var\", \"vision_model.features.5.1.block.0.0.weight\", \"vision_model.features.5.1.block.0.1.weight\", \"vision_model.features.5.1.block.0.1.bias\", \"vision_model.features.5.1.block.0.1.running_mean\", \"vision_model.features.5.1.block.0.1.running_var\", \"vision_model.features.5.1.block.1.0.weight\", \"vision_model.features.5.1.block.1.1.weight\", \"vision_model.features.5.1.block.1.1.bias\", \"vision_model.features.5.1.block.1.1.running_mean\", \"vision_model.features.5.1.block.1.1.running_var\", \"vision_model.features.5.1.block.2.fc1.weight\", \"vision_model.features.5.1.block.2.fc1.bias\", \"vision_model.features.5.1.block.2.fc2.weight\", \"vision_model.features.5.1.block.2.fc2.bias\", \"vision_model.features.5.1.block.3.0.weight\", \"vision_model.features.5.1.block.3.1.weight\", \"vision_model.features.5.1.block.3.1.bias\", \"vision_model.features.5.1.block.3.1.running_mean\", \"vision_model.features.5.1.block.3.1.running_var\", \"vision_model.features.5.2.block.0.0.weight\", \"vision_model.features.5.2.block.0.1.weight\", \"vision_model.features.5.2.block.0.1.bias\", \"vision_model.features.5.2.block.0.1.running_mean\", \"vision_model.features.5.2.block.0.1.running_var\", \"vision_model.features.5.2.block.1.0.weight\", \"vision_model.features.5.2.block.1.1.weight\", \"vision_model.features.5.2.block.1.1.bias\", \"vision_model.features.5.2.block.1.1.running_mean\", \"vision_model.features.5.2.block.1.1.running_var\", \"vision_model.features.5.2.block.2.fc1.weight\", \"vision_model.features.5.2.block.2.fc1.bias\", \"vision_model.features.5.2.block.2.fc2.weight\", \"vision_model.features.5.2.block.2.fc2.bias\", \"vision_model.features.5.2.block.3.0.weight\", \"vision_model.features.5.2.block.3.1.weight\", \"vision_model.features.5.2.block.3.1.bias\", \"vision_model.features.5.2.block.3.1.running_mean\", \"vision_model.features.5.2.block.3.1.running_var\", \"vision_model.features.5.3.block.0.0.weight\", \"vision_model.features.5.3.block.0.1.weight\", \"vision_model.features.5.3.block.0.1.bias\", \"vision_model.features.5.3.block.0.1.running_mean\", \"vision_model.features.5.3.block.0.1.running_var\", \"vision_model.features.5.3.block.1.0.weight\", \"vision_model.features.5.3.block.1.1.weight\", \"vision_model.features.5.3.block.1.1.bias\", \"vision_model.features.5.3.block.1.1.running_mean\", \"vision_model.features.5.3.block.1.1.running_var\", \"vision_model.features.5.3.block.2.fc1.weight\", \"vision_model.features.5.3.block.2.fc1.bias\", \"vision_model.features.5.3.block.2.fc2.weight\", \"vision_model.features.5.3.block.2.fc2.bias\", \"vision_model.features.5.3.block.3.0.weight\", \"vision_model.features.5.3.block.3.1.weight\", \"vision_model.features.5.3.block.3.1.bias\", \"vision_model.features.5.3.block.3.1.running_mean\", \"vision_model.features.5.3.block.3.1.running_var\", \"vision_model.features.5.4.block.0.0.weight\", \"vision_model.features.5.4.block.0.1.weight\", \"vision_model.features.5.4.block.0.1.bias\", \"vision_model.features.5.4.block.0.1.running_mean\", \"vision_model.features.5.4.block.0.1.running_var\", \"vision_model.features.5.4.block.1.0.weight\", \"vision_model.features.5.4.block.1.1.weight\", \"vision_model.features.5.4.block.1.1.bias\", \"vision_model.features.5.4.block.1.1.running_mean\", \"vision_model.features.5.4.block.1.1.running_var\", \"vision_model.features.5.4.block.2.fc1.weight\", \"vision_model.features.5.4.block.2.fc1.bias\", \"vision_model.features.5.4.block.2.fc2.weight\", \"vision_model.features.5.4.block.2.fc2.bias\", \"vision_model.features.5.4.block.3.0.weight\", \"vision_model.features.5.4.block.3.1.weight\", \"vision_model.features.5.4.block.3.1.bias\", \"vision_model.features.5.4.block.3.1.running_mean\", \"vision_model.features.5.4.block.3.1.running_var\", \"vision_model.features.5.5.block.0.0.weight\", \"vision_model.features.5.5.block.0.1.weight\", \"vision_model.features.5.5.block.0.1.bias\", \"vision_model.features.5.5.block.0.1.running_mean\", \"vision_model.features.5.5.block.0.1.running_var\", \"vision_model.features.5.5.block.1.0.weight\", \"vision_model.features.5.5.block.1.1.weight\", \"vision_model.features.5.5.block.1.1.bias\", \"vision_model.features.5.5.block.1.1.running_mean\", \"vision_model.features.5.5.block.1.1.running_var\", \"vision_model.features.5.5.block.2.fc1.weight\", \"vision_model.features.5.5.block.2.fc1.bias\", \"vision_model.features.5.5.block.2.fc2.weight\", \"vision_model.features.5.5.block.2.fc2.bias\", \"vision_model.features.5.5.block.3.0.weight\", \"vision_model.features.5.5.block.3.1.weight\", \"vision_model.features.5.5.block.3.1.bias\", \"vision_model.features.5.5.block.3.1.running_mean\", \"vision_model.features.5.5.block.3.1.running_var\", \"vision_model.features.5.6.block.0.0.weight\", \"vision_model.features.5.6.block.0.1.weight\", \"vision_model.features.5.6.block.0.1.bias\", \"vision_model.features.5.6.block.0.1.running_mean\", \"vision_model.features.5.6.block.0.1.running_var\", \"vision_model.features.5.6.block.1.0.weight\", \"vision_model.features.5.6.block.1.1.weight\", \"vision_model.features.5.6.block.1.1.bias\", \"vision_model.features.5.6.block.1.1.running_mean\", \"vision_model.features.5.6.block.1.1.running_var\", \"vision_model.features.5.6.block.2.fc1.weight\", \"vision_model.features.5.6.block.2.fc1.bias\", \"vision_model.features.5.6.block.2.fc2.weight\", \"vision_model.features.5.6.block.2.fc2.bias\", \"vision_model.features.5.6.block.3.0.weight\", \"vision_model.features.5.6.block.3.1.weight\", \"vision_model.features.5.6.block.3.1.bias\", \"vision_model.features.5.6.block.3.1.running_mean\", \"vision_model.features.5.6.block.3.1.running_var\", \"vision_model.features.5.7.block.0.0.weight\", \"vision_model.features.5.7.block.0.1.weight\", \"vision_model.features.5.7.block.0.1.bias\", \"vision_model.features.5.7.block.0.1.running_mean\", \"vision_model.features.5.7.block.0.1.running_var\", \"vision_model.features.5.7.block.1.0.weight\", \"vision_model.features.5.7.block.1.1.weight\", \"vision_model.features.5.7.block.1.1.bias\", \"vision_model.features.5.7.block.1.1.running_mean\", \"vision_model.features.5.7.block.1.1.running_var\", \"vision_model.features.5.7.block.2.fc1.weight\", \"vision_model.features.5.7.block.2.fc1.bias\", \"vision_model.features.5.7.block.2.fc2.weight\", \"vision_model.features.5.7.block.2.fc2.bias\", \"vision_model.features.5.7.block.3.0.weight\", \"vision_model.features.5.7.block.3.1.weight\", \"vision_model.features.5.7.block.3.1.bias\", \"vision_model.features.5.7.block.3.1.running_mean\", \"vision_model.features.5.7.block.3.1.running_var\", \"vision_model.features.5.8.block.0.0.weight\", \"vision_model.features.5.8.block.0.1.weight\", \"vision_model.features.5.8.block.0.1.bias\", \"vision_model.features.5.8.block.0.1.running_mean\", \"vision_model.features.5.8.block.0.1.running_var\", \"vision_model.features.5.8.block.1.0.weight\", \"vision_model.features.5.8.block.1.1.weight\", \"vision_model.features.5.8.block.1.1.bias\", \"vision_model.features.5.8.block.1.1.running_mean\", \"vision_model.features.5.8.block.1.1.running_var\", \"vision_model.features.5.8.block.2.fc1.weight\", \"vision_model.features.5.8.block.2.fc1.bias\", \"vision_model.features.5.8.block.2.fc2.weight\", \"vision_model.features.5.8.block.2.fc2.bias\", \"vision_model.features.5.8.block.3.0.weight\", \"vision_model.features.5.8.block.3.1.weight\", \"vision_model.features.5.8.block.3.1.bias\", \"vision_model.features.5.8.block.3.1.running_mean\", \"vision_model.features.5.8.block.3.1.running_var\", \"vision_model.features.6.0.block.0.0.weight\", \"vision_model.features.6.0.block.0.1.weight\", \"vision_model.features.6.0.block.0.1.bias\", \"vision_model.features.6.0.block.0.1.running_mean\", \"vision_model.features.6.0.block.0.1.running_var\", \"vision_model.features.6.0.block.1.0.weight\", \"vision_model.features.6.0.block.1.1.weight\", \"vision_model.features.6.0.block.1.1.bias\", \"vision_model.features.6.0.block.1.1.running_mean\", \"vision_model.features.6.0.block.1.1.running_var\", \"vision_model.features.6.0.block.2.fc1.weight\", \"vision_model.features.6.0.block.2.fc1.bias\", \"vision_model.features.6.0.block.2.fc2.weight\", \"vision_model.features.6.0.block.2.fc2.bias\", \"vision_model.features.6.0.block.3.0.weight\", \"vision_model.features.6.0.block.3.1.weight\", \"vision_model.features.6.0.block.3.1.bias\", \"vision_model.features.6.0.block.3.1.running_mean\", \"vision_model.features.6.0.block.3.1.running_var\", \"vision_model.features.6.1.block.0.0.weight\", \"vision_model.features.6.1.block.0.1.weight\", \"vision_model.features.6.1.block.0.1.bias\", \"vision_model.features.6.1.block.0.1.running_mean\", \"vision_model.features.6.1.block.0.1.running_var\", \"vision_model.features.6.1.block.1.0.weight\", \"vision_model.features.6.1.block.1.1.weight\", \"vision_model.features.6.1.block.1.1.bias\", \"vision_model.features.6.1.block.1.1.running_mean\", \"vision_model.features.6.1.block.1.1.running_var\", \"vision_model.features.6.1.block.2.fc1.weight\", \"vision_model.features.6.1.block.2.fc1.bias\", \"vision_model.features.6.1.block.2.fc2.weight\", \"vision_model.features.6.1.block.2.fc2.bias\", \"vision_model.features.6.1.block.3.0.weight\", \"vision_model.features.6.1.block.3.1.weight\", \"vision_model.features.6.1.block.3.1.bias\", \"vision_model.features.6.1.block.3.1.running_mean\", \"vision_model.features.6.1.block.3.1.running_var\", \"vision_model.features.6.2.block.0.0.weight\", \"vision_model.features.6.2.block.0.1.weight\", \"vision_model.features.6.2.block.0.1.bias\", \"vision_model.features.6.2.block.0.1.running_mean\", \"vision_model.features.6.2.block.0.1.running_var\", \"vision_model.features.6.2.block.1.0.weight\", \"vision_model.features.6.2.block.1.1.weight\", \"vision_model.features.6.2.block.1.1.bias\", \"vision_model.features.6.2.block.1.1.running_mean\", \"vision_model.features.6.2.block.1.1.running_var\", \"vision_model.features.6.2.block.2.fc1.weight\", \"vision_model.features.6.2.block.2.fc1.bias\", \"vision_model.features.6.2.block.2.fc2.weight\", \"vision_model.features.6.2.block.2.fc2.bias\", \"vision_model.features.6.2.block.3.0.weight\", \"vision_model.features.6.2.block.3.1.weight\", \"vision_model.features.6.2.block.3.1.bias\", \"vision_model.features.6.2.block.3.1.running_mean\", \"vision_model.features.6.2.block.3.1.running_var\", \"vision_model.features.6.3.block.0.0.weight\", \"vision_model.features.6.3.block.0.1.weight\", \"vision_model.features.6.3.block.0.1.bias\", \"vision_model.features.6.3.block.0.1.running_mean\", \"vision_model.features.6.3.block.0.1.running_var\", \"vision_model.features.6.3.block.1.0.weight\", \"vision_model.features.6.3.block.1.1.weight\", \"vision_model.features.6.3.block.1.1.bias\", \"vision_model.features.6.3.block.1.1.running_mean\", \"vision_model.features.6.3.block.1.1.running_var\", \"vision_model.features.6.3.block.2.fc1.weight\", \"vision_model.features.6.3.block.2.fc1.bias\", \"vision_model.features.6.3.block.2.fc2.weight\", \"vision_model.features.6.3.block.2.fc2.bias\", \"vision_model.features.6.3.block.3.0.weight\", \"vision_model.features.6.3.block.3.1.weight\", \"vision_model.features.6.3.block.3.1.bias\", \"vision_model.features.6.3.block.3.1.running_mean\", \"vision_model.features.6.3.block.3.1.running_var\", \"vision_model.features.6.4.block.0.0.weight\", \"vision_model.features.6.4.block.0.1.weight\", \"vision_model.features.6.4.block.0.1.bias\", \"vision_model.features.6.4.block.0.1.running_mean\", \"vision_model.features.6.4.block.0.1.running_var\", \"vision_model.features.6.4.block.1.0.weight\", \"vision_model.features.6.4.block.1.1.weight\", \"vision_model.features.6.4.block.1.1.bias\", \"vision_model.features.6.4.block.1.1.running_mean\", \"vision_model.features.6.4.block.1.1.running_var\", \"vision_model.features.6.4.block.2.fc1.weight\", \"vision_model.features.6.4.block.2.fc1.bias\", \"vision_model.features.6.4.block.2.fc2.weight\", \"vision_model.features.6.4.block.2.fc2.bias\", \"vision_model.features.6.4.block.3.0.weight\", \"vision_model.features.6.4.block.3.1.weight\", \"vision_model.features.6.4.block.3.1.bias\", \"vision_model.features.6.4.block.3.1.running_mean\", \"vision_model.features.6.4.block.3.1.running_var\", \"vision_model.features.6.5.block.0.0.weight\", \"vision_model.features.6.5.block.0.1.weight\", \"vision_model.features.6.5.block.0.1.bias\", \"vision_model.features.6.5.block.0.1.running_mean\", \"vision_model.features.6.5.block.0.1.running_var\", \"vision_model.features.6.5.block.1.0.weight\", \"vision_model.features.6.5.block.1.1.weight\", \"vision_model.features.6.5.block.1.1.bias\", \"vision_model.features.6.5.block.1.1.running_mean\", \"vision_model.features.6.5.block.1.1.running_var\", \"vision_model.features.6.5.block.2.fc1.weight\", \"vision_model.features.6.5.block.2.fc1.bias\", \"vision_model.features.6.5.block.2.fc2.weight\", \"vision_model.features.6.5.block.2.fc2.bias\", \"vision_model.features.6.5.block.3.0.weight\", \"vision_model.features.6.5.block.3.1.weight\", \"vision_model.features.6.5.block.3.1.bias\", \"vision_model.features.6.5.block.3.1.running_mean\", \"vision_model.features.6.5.block.3.1.running_var\", \"vision_model.features.6.6.block.0.0.weight\", \"vision_model.features.6.6.block.0.1.weight\", \"vision_model.features.6.6.block.0.1.bias\", \"vision_model.features.6.6.block.0.1.running_mean\", \"vision_model.features.6.6.block.0.1.running_var\", \"vision_model.features.6.6.block.1.0.weight\", \"vision_model.features.6.6.block.1.1.weight\", \"vision_model.features.6.6.block.1.1.bias\", \"vision_model.features.6.6.block.1.1.running_mean\", \"vision_model.features.6.6.block.1.1.running_var\", \"vision_model.features.6.6.block.2.fc1.weight\", \"vision_model.features.6.6.block.2.fc1.bias\", \"vision_model.features.6.6.block.2.fc2.weight\", \"vision_model.features.6.6.block.2.fc2.bias\", \"vision_model.features.6.6.block.3.0.weight\", \"vision_model.features.6.6.block.3.1.weight\", \"vision_model.features.6.6.block.3.1.bias\", \"vision_model.features.6.6.block.3.1.running_mean\", \"vision_model.features.6.6.block.3.1.running_var\", \"vision_model.features.6.7.block.0.0.weight\", \"vision_model.features.6.7.block.0.1.weight\", \"vision_model.features.6.7.block.0.1.bias\", \"vision_model.features.6.7.block.0.1.running_mean\", \"vision_model.features.6.7.block.0.1.running_var\", \"vision_model.features.6.7.block.1.0.weight\", \"vision_model.features.6.7.block.1.1.weight\", \"vision_model.features.6.7.block.1.1.bias\", \"vision_model.features.6.7.block.1.1.running_mean\", \"vision_model.features.6.7.block.1.1.running_var\", \"vision_model.features.6.7.block.2.fc1.weight\", \"vision_model.features.6.7.block.2.fc1.bias\", \"vision_model.features.6.7.block.2.fc2.weight\", \"vision_model.features.6.7.block.2.fc2.bias\", \"vision_model.features.6.7.block.3.0.weight\", \"vision_model.features.6.7.block.3.1.weight\", \"vision_model.features.6.7.block.3.1.bias\", \"vision_model.features.6.7.block.3.1.running_mean\", \"vision_model.features.6.7.block.3.1.running_var\", \"vision_model.features.6.8.block.0.0.weight\", \"vision_model.features.6.8.block.0.1.weight\", \"vision_model.features.6.8.block.0.1.bias\", \"vision_model.features.6.8.block.0.1.running_mean\", \"vision_model.features.6.8.block.0.1.running_var\", \"vision_model.features.6.8.block.1.0.weight\", \"vision_model.features.6.8.block.1.1.weight\", \"vision_model.features.6.8.block.1.1.bias\", \"vision_model.features.6.8.block.1.1.running_mean\", \"vision_model.features.6.8.block.1.1.running_var\", \"vision_model.features.6.8.block.2.fc1.weight\", \"vision_model.features.6.8.block.2.fc1.bias\", \"vision_model.features.6.8.block.2.fc2.weight\", \"vision_model.features.6.8.block.2.fc2.bias\", \"vision_model.features.6.8.block.3.0.weight\", \"vision_model.features.6.8.block.3.1.weight\", \"vision_model.features.6.8.block.3.1.bias\", \"vision_model.features.6.8.block.3.1.running_mean\", \"vision_model.features.6.8.block.3.1.running_var\", \"vision_model.features.6.9.block.0.0.weight\", \"vision_model.features.6.9.block.0.1.weight\", \"vision_model.features.6.9.block.0.1.bias\", \"vision_model.features.6.9.block.0.1.running_mean\", \"vision_model.features.6.9.block.0.1.running_var\", \"vision_model.features.6.9.block.1.0.weight\", \"vision_model.features.6.9.block.1.1.weight\", \"vision_model.features.6.9.block.1.1.bias\", \"vision_model.features.6.9.block.1.1.running_mean\", \"vision_model.features.6.9.block.1.1.running_var\", \"vision_model.features.6.9.block.2.fc1.weight\", \"vision_model.features.6.9.block.2.fc1.bias\", \"vision_model.features.6.9.block.2.fc2.weight\", \"vision_model.features.6.9.block.2.fc2.bias\", \"vision_model.features.6.9.block.3.0.weight\", \"vision_model.features.6.9.block.3.1.weight\", \"vision_model.features.6.9.block.3.1.bias\", \"vision_model.features.6.9.block.3.1.running_mean\", \"vision_model.features.6.9.block.3.1.running_var\", \"vision_model.features.6.10.block.0.0.weight\", \"vision_model.features.6.10.block.0.1.weight\", \"vision_model.features.6.10.block.0.1.bias\", \"vision_model.features.6.10.block.0.1.running_mean\", \"vision_model.features.6.10.block.0.1.running_var\", \"vision_model.features.6.10.block.1.0.weight\", \"vision_model.features.6.10.block.1.1.weight\", \"vision_model.features.6.10.block.1.1.bias\", \"vision_model.features.6.10.block.1.1.running_mean\", \"vision_model.features.6.10.block.1.1.running_var\", \"vision_model.features.6.10.block.2.fc1.weight\", \"vision_model.features.6.10.block.2.fc1.bias\", \"vision_model.features.6.10.block.2.fc2.weight\", \"vision_model.features.6.10.block.2.fc2.bias\", \"vision_model.features.6.10.block.3.0.weight\", \"vision_model.features.6.10.block.3.1.weight\", \"vision_model.features.6.10.block.3.1.bias\", \"vision_model.features.6.10.block.3.1.running_mean\", \"vision_model.features.6.10.block.3.1.running_var\", \"vision_model.features.6.11.block.0.0.weight\", \"vision_model.features.6.11.block.0.1.weight\", \"vision_model.features.6.11.block.0.1.bias\", \"vision_model.features.6.11.block.0.1.running_mean\", \"vision_model.features.6.11.block.0.1.running_var\", \"vision_model.features.6.11.block.1.0.weight\", \"vision_model.features.6.11.block.1.1.weight\", \"vision_model.features.6.11.block.1.1.bias\", \"vision_model.features.6.11.block.1.1.running_mean\", \"vision_model.features.6.11.block.1.1.running_var\", \"vision_model.features.6.11.block.2.fc1.weight\", \"vision_model.features.6.11.block.2.fc1.bias\", \"vision_model.features.6.11.block.2.fc2.weight\", \"vision_model.features.6.11.block.2.fc2.bias\", \"vision_model.features.6.11.block.3.0.weight\", \"vision_model.features.6.11.block.3.1.weight\", \"vision_model.features.6.11.block.3.1.bias\", \"vision_model.features.6.11.block.3.1.running_mean\", \"vision_model.features.6.11.block.3.1.running_var\", \"vision_model.features.6.12.block.0.0.weight\", \"vision_model.features.6.12.block.0.1.weight\", \"vision_model.features.6.12.block.0.1.bias\", \"vision_model.features.6.12.block.0.1.running_mean\", \"vision_model.features.6.12.block.0.1.running_var\", \"vision_model.features.6.12.block.1.0.weight\", \"vision_model.features.6.12.block.1.1.weight\", \"vision_model.features.6.12.block.1.1.bias\", \"vision_model.features.6.12.block.1.1.running_mean\", \"vision_model.features.6.12.block.1.1.running_var\", \"vision_model.features.6.12.block.2.fc1.weight\", \"vision_model.features.6.12.block.2.fc1.bias\", \"vision_model.features.6.12.block.2.fc2.weight\", \"vision_model.features.6.12.block.2.fc2.bias\", \"vision_model.features.6.12.block.3.0.weight\", \"vision_model.features.6.12.block.3.1.weight\", \"vision_model.features.6.12.block.3.1.bias\", \"vision_model.features.6.12.block.3.1.running_mean\", \"vision_model.features.6.12.block.3.1.running_var\", \"vision_model.features.6.13.block.0.0.weight\", \"vision_model.features.6.13.block.0.1.weight\", \"vision_model.features.6.13.block.0.1.bias\", \"vision_model.features.6.13.block.0.1.running_mean\", \"vision_model.features.6.13.block.0.1.running_var\", \"vision_model.features.6.13.block.1.0.weight\", \"vision_model.features.6.13.block.1.1.weight\", \"vision_model.features.6.13.block.1.1.bias\", \"vision_model.features.6.13.block.1.1.running_mean\", \"vision_model.features.6.13.block.1.1.running_var\", \"vision_model.features.6.13.block.2.fc1.weight\", \"vision_model.features.6.13.block.2.fc1.bias\", \"vision_model.features.6.13.block.2.fc2.weight\", \"vision_model.features.6.13.block.2.fc2.bias\", \"vision_model.features.6.13.block.3.0.weight\", \"vision_model.features.6.13.block.3.1.weight\", \"vision_model.features.6.13.block.3.1.bias\", \"vision_model.features.6.13.block.3.1.running_mean\", \"vision_model.features.6.13.block.3.1.running_var\", \"vision_model.features.6.14.block.0.0.weight\", \"vision_model.features.6.14.block.0.1.weight\", \"vision_model.features.6.14.block.0.1.bias\", \"vision_model.features.6.14.block.0.1.running_mean\", \"vision_model.features.6.14.block.0.1.running_var\", \"vision_model.features.6.14.block.1.0.weight\", \"vision_model.features.6.14.block.1.1.weight\", \"vision_model.features.6.14.block.1.1.bias\", \"vision_model.features.6.14.block.1.1.running_mean\", \"vision_model.features.6.14.block.1.1.running_var\", \"vision_model.features.6.14.block.2.fc1.weight\", \"vision_model.features.6.14.block.2.fc1.bias\", \"vision_model.features.6.14.block.2.fc2.weight\", \"vision_model.features.6.14.block.2.fc2.bias\", \"vision_model.features.6.14.block.3.0.weight\", \"vision_model.features.6.14.block.3.1.weight\", \"vision_model.features.6.14.block.3.1.bias\", \"vision_model.features.6.14.block.3.1.running_mean\", \"vision_model.features.6.14.block.3.1.running_var\", \"vision_model.features.7.0.weight\", \"vision_model.features.7.1.weight\", \"vision_model.features.7.1.bias\", \"vision_model.features.7.1.running_mean\", \"vision_model.features.7.1.running_var\", \"text_model.embeddings.word_embeddings.weight\", \"text_model.embeddings.LayerNorm.weight\", \"text_model.embeddings.LayerNorm.bias\", \"text_model.encoder.layer.0.attention.self.q_bias\", \"text_model.encoder.layer.0.attention.self.v_bias\", \"text_model.encoder.layer.0.attention.self.in_proj.weight\", \"text_model.encoder.layer.0.attention.self.pos_proj.weight\", \"text_model.encoder.layer.0.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.0.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.0.attention.output.dense.weight\", \"text_model.encoder.layer.0.attention.output.dense.bias\", \"text_model.encoder.layer.0.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.0.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.0.intermediate.dense.weight\", \"text_model.encoder.layer.0.intermediate.dense.bias\", \"text_model.encoder.layer.0.output.dense.weight\", \"text_model.encoder.layer.0.output.dense.bias\", \"text_model.encoder.layer.0.output.LayerNorm.weight\", \"text_model.encoder.layer.0.output.LayerNorm.bias\", \"text_model.encoder.layer.1.attention.self.q_bias\", \"text_model.encoder.layer.1.attention.self.v_bias\", \"text_model.encoder.layer.1.attention.self.in_proj.weight\", \"text_model.encoder.layer.1.attention.self.pos_proj.weight\", \"text_model.encoder.layer.1.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.1.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.1.attention.output.dense.weight\", \"text_model.encoder.layer.1.attention.output.dense.bias\", \"text_model.encoder.layer.1.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.1.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.1.intermediate.dense.weight\", \"text_model.encoder.layer.1.intermediate.dense.bias\", \"text_model.encoder.layer.1.output.dense.weight\", \"text_model.encoder.layer.1.output.dense.bias\", \"text_model.encoder.layer.1.output.LayerNorm.weight\", \"text_model.encoder.layer.1.output.LayerNorm.bias\", \"text_model.encoder.layer.2.attention.self.q_bias\", \"text_model.encoder.layer.2.attention.self.v_bias\", \"text_model.encoder.layer.2.attention.self.in_proj.weight\", \"text_model.encoder.layer.2.attention.self.pos_proj.weight\", \"text_model.encoder.layer.2.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.2.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.2.attention.output.dense.weight\", \"text_model.encoder.layer.2.attention.output.dense.bias\", \"text_model.encoder.layer.2.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.2.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.2.intermediate.dense.weight\", \"text_model.encoder.layer.2.intermediate.dense.bias\", \"text_model.encoder.layer.2.output.dense.weight\", \"text_model.encoder.layer.2.output.dense.bias\", \"text_model.encoder.layer.2.output.LayerNorm.weight\", \"text_model.encoder.layer.2.output.LayerNorm.bias\", \"text_model.encoder.layer.3.attention.self.q_bias\", \"text_model.encoder.layer.3.attention.self.v_bias\", \"text_model.encoder.layer.3.attention.self.in_proj.weight\", \"text_model.encoder.layer.3.attention.self.pos_proj.weight\", \"text_model.encoder.layer.3.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.3.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.3.attention.output.dense.weight\", \"text_model.encoder.layer.3.attention.output.dense.bias\", \"text_model.encoder.layer.3.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.3.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.3.intermediate.dense.weight\", \"text_model.encoder.layer.3.intermediate.dense.bias\", \"text_model.encoder.layer.3.output.dense.weight\", \"text_model.encoder.layer.3.output.dense.bias\", \"text_model.encoder.layer.3.output.LayerNorm.weight\", \"text_model.encoder.layer.3.output.LayerNorm.bias\", \"text_model.encoder.layer.4.attention.self.q_bias\", \"text_model.encoder.layer.4.attention.self.v_bias\", \"text_model.encoder.layer.4.attention.self.in_proj.weight\", \"text_model.encoder.layer.4.attention.self.pos_proj.weight\", \"text_model.encoder.layer.4.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.4.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.4.attention.output.dense.weight\", \"text_model.encoder.layer.4.attention.output.dense.bias\", \"text_model.encoder.layer.4.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.4.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.4.intermediate.dense.weight\", \"text_model.encoder.layer.4.intermediate.dense.bias\", \"text_model.encoder.layer.4.output.dense.weight\", \"text_model.encoder.layer.4.output.dense.bias\", \"text_model.encoder.layer.4.output.LayerNorm.weight\", \"text_model.encoder.layer.4.output.LayerNorm.bias\", \"text_model.encoder.layer.5.attention.self.q_bias\", \"text_model.encoder.layer.5.attention.self.v_bias\", \"text_model.encoder.layer.5.attention.self.in_proj.weight\", \"text_model.encoder.layer.5.attention.self.pos_proj.weight\", \"text_model.encoder.layer.5.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.5.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.5.attention.output.dense.weight\", \"text_model.encoder.layer.5.attention.output.dense.bias\", \"text_model.encoder.layer.5.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.5.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.5.intermediate.dense.weight\", \"text_model.encoder.layer.5.intermediate.dense.bias\", \"text_model.encoder.layer.5.output.dense.weight\", \"text_model.encoder.layer.5.output.dense.bias\", \"text_model.encoder.layer.5.output.LayerNorm.weight\", \"text_model.encoder.layer.5.output.LayerNorm.bias\", \"text_model.encoder.layer.6.attention.self.q_bias\", \"text_model.encoder.layer.6.attention.self.v_bias\", \"text_model.encoder.layer.6.attention.self.in_proj.weight\", \"text_model.encoder.layer.6.attention.self.pos_proj.weight\", \"text_model.encoder.layer.6.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.6.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.6.attention.output.dense.weight\", \"text_model.encoder.layer.6.attention.output.dense.bias\", \"text_model.encoder.layer.6.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.6.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.6.intermediate.dense.weight\", \"text_model.encoder.layer.6.intermediate.dense.bias\", \"text_model.encoder.layer.6.output.dense.weight\", \"text_model.encoder.layer.6.output.dense.bias\", \"text_model.encoder.layer.6.output.LayerNorm.weight\", \"text_model.encoder.layer.6.output.LayerNorm.bias\", \"text_model.encoder.layer.7.attention.self.q_bias\", \"text_model.encoder.layer.7.attention.self.v_bias\", \"text_model.encoder.layer.7.attention.self.in_proj.weight\", \"text_model.encoder.layer.7.attention.self.pos_proj.weight\", \"text_model.encoder.layer.7.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.7.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.7.attention.output.dense.weight\", \"text_model.encoder.layer.7.attention.output.dense.bias\", \"text_model.encoder.layer.7.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.7.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.7.intermediate.dense.weight\", \"text_model.encoder.layer.7.intermediate.dense.bias\", \"text_model.encoder.layer.7.output.dense.weight\", \"text_model.encoder.layer.7.output.dense.bias\", \"text_model.encoder.layer.7.output.LayerNorm.weight\", \"text_model.encoder.layer.7.output.LayerNorm.bias\", \"text_model.encoder.layer.8.attention.self.q_bias\", \"text_model.encoder.layer.8.attention.self.v_bias\", \"text_model.encoder.layer.8.attention.self.in_proj.weight\", \"text_model.encoder.layer.8.attention.self.pos_proj.weight\", \"text_model.encoder.layer.8.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.8.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.8.attention.output.dense.weight\", \"text_model.encoder.layer.8.attention.output.dense.bias\", \"text_model.encoder.layer.8.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.8.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.8.intermediate.dense.weight\", \"text_model.encoder.layer.8.intermediate.dense.bias\", \"text_model.encoder.layer.8.output.dense.weight\", \"text_model.encoder.layer.8.output.dense.bias\", \"text_model.encoder.layer.8.output.LayerNorm.weight\", \"text_model.encoder.layer.8.output.LayerNorm.bias\", \"text_model.encoder.layer.9.attention.self.q_bias\", \"text_model.encoder.layer.9.attention.self.v_bias\", \"text_model.encoder.layer.9.attention.self.in_proj.weight\", \"text_model.encoder.layer.9.attention.self.pos_proj.weight\", \"text_model.encoder.layer.9.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.9.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.9.attention.output.dense.weight\", \"text_model.encoder.layer.9.attention.output.dense.bias\", \"text_model.encoder.layer.9.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.9.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.9.intermediate.dense.weight\", \"text_model.encoder.layer.9.intermediate.dense.bias\", \"text_model.encoder.layer.9.output.dense.weight\", \"text_model.encoder.layer.9.output.dense.bias\", \"text_model.encoder.layer.9.output.LayerNorm.weight\", \"text_model.encoder.layer.9.output.LayerNorm.bias\", \"text_model.encoder.layer.10.attention.self.q_bias\", \"text_model.encoder.layer.10.attention.self.v_bias\", \"text_model.encoder.layer.10.attention.self.in_proj.weight\", \"text_model.encoder.layer.10.attention.self.pos_proj.weight\", \"text_model.encoder.layer.10.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.10.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.10.attention.output.dense.weight\", \"text_model.encoder.layer.10.attention.output.dense.bias\", \"text_model.encoder.layer.10.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.10.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.10.intermediate.dense.weight\", \"text_model.encoder.layer.10.intermediate.dense.bias\", \"text_model.encoder.layer.10.output.dense.weight\", \"text_model.encoder.layer.10.output.dense.bias\", \"text_model.encoder.layer.10.output.LayerNorm.weight\", \"text_model.encoder.layer.10.output.LayerNorm.bias\", \"text_model.encoder.layer.11.attention.self.q_bias\", \"text_model.encoder.layer.11.attention.self.v_bias\", \"text_model.encoder.layer.11.attention.self.in_proj.weight\", \"text_model.encoder.layer.11.attention.self.pos_proj.weight\", \"text_model.encoder.layer.11.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.11.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.11.attention.output.dense.weight\", \"text_model.encoder.layer.11.attention.output.dense.bias\", \"text_model.encoder.layer.11.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.11.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.11.intermediate.dense.weight\", \"text_model.encoder.layer.11.intermediate.dense.bias\", \"text_model.encoder.layer.11.output.dense.weight\", \"text_model.encoder.layer.11.output.dense.bias\", \"text_model.encoder.layer.11.output.LayerNorm.weight\", \"text_model.encoder.layer.11.output.LayerNorm.bias\", \"text_model.encoder.rel_embeddings.weight\", \"cross_attention.image_proj.weight\", \"cross_attention.image_proj.bias\", \"cross_attention.text_proj.weight\", \"cross_attention.text_proj.bias\", \"cross_attention.attention.in_proj_weight\", \"cross_attention.attention.in_proj_bias\", \"cross_attention.attention.out_proj.weight\", \"cross_attention.attention.out_proj.bias\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.3.weight\", \"classifier.3.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"model_state_dict\", \"optimizer_state_dict\", \"scheduler_state_dict\", \"loss\". "],"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for MultimodalModel:\n\tMissing key(s) in state_dict: \"vision_model.features.0.0.weight\", \"vision_model.features.0.1.weight\", \"vision_model.features.0.1.bias\", \"vision_model.features.0.1.running_mean\", \"vision_model.features.0.1.running_var\", \"vision_model.features.1.0.block.0.0.weight\", \"vision_model.features.1.0.block.0.1.weight\", \"vision_model.features.1.0.block.0.1.bias\", \"vision_model.features.1.0.block.0.1.running_mean\", \"vision_model.features.1.0.block.0.1.running_var\", \"vision_model.features.1.1.block.0.0.weight\", \"vision_model.features.1.1.block.0.1.weight\", \"vision_model.features.1.1.block.0.1.bias\", \"vision_model.features.1.1.block.0.1.running_mean\", \"vision_model.features.1.1.block.0.1.running_var\", \"vision_model.features.2.0.block.0.0.weight\", \"vision_model.features.2.0.block.0.1.weight\", \"vision_model.features.2.0.block.0.1.bias\", \"vision_model.features.2.0.block.0.1.running_mean\", \"vision_model.features.2.0.block.0.1.running_var\", \"vision_model.features.2.0.block.1.0.weight\", \"vision_model.features.2.0.block.1.1.weight\", \"vision_model.features.2.0.block.1.1.bias\", \"vision_model.features.2.0.block.1.1.running_mean\", \"vision_model.features.2.0.block.1.1.running_var\", \"vision_model.features.2.1.block.0.0.weight\", \"vision_model.features.2.1.block.0.1.weight\", \"vision_model.features.2.1.block.0.1.bias\", \"vision_model.features.2.1.block.0.1.running_mean\", \"vision_model.features.2.1.block.0.1.running_var\", \"vision_model.features.2.1.block.1.0.weight\", \"vision_model.features.2.1.block.1.1.weight\", \"vision_model.features.2.1.block.1.1.bias\", \"vision_model.features.2.1.block.1.1.running_mean\", \"vision_model.features.2.1.block.1.1.running_var\", \"vision_model.features.2.2.block.0.0.weight\", \"vision_model.features.2.2.block.0.1.weight\", \"vision_model.features.2.2.block.0.1.bias\", \"vision_model.features.2.2.block.0.1.running_mean\", \"vision_model.features.2.2.block.0.1.running_var\", \"vision_model.features.2.2.block.1.0.weight\", \"vision_model.features.2.2.block.1.1.weight\", \"vision_model.features.2.2.block.1.1.bias\", \"vision_model.features.2.2.block.1.1.running_mean\", \"vision_model.features.2.2.block.1.1.running_var\", \"vision_model.features.2.3.block.0.0.weight\", \"vision_model.features.2.3.block.0.1.weight\", \"vision_model.features.2.3.block.0.1.bias\", \"vision_model.features.2.3.block.0.1.running_mean\", \"vision_model.features.2.3.block.0.1.running_var\", \"vision_model.features.2.3.block.1.0.weight\", \"vision_model.features.2.3.block.1.1.weight\", \"vision_model.features.2.3.block.1.1.bias\", \"vision_model.features.2.3.block.1.1.running_mean\", \"vision_model.features.2.3.block.1.1.running_var\", \"vision_model.features.3.0.block.0.0.weight\", \"vision_model.features.3.0.block.0.1.weight\", \"vision_model.features.3.0.block.0.1.bias\", \"vision_model.features.3.0.block.0.1.running_mean\", \"vision_model.features.3.0.block.0.1.running_var\", \"vision_model.features.3.0.block.1.0.weight\", \"vision_model.features.3.0.block.1.1.weight\", \"vision_model.features.3.0.block.1.1.bias\", \"vision_model.features.3.0.block.1.1.running_mean\", \"vision_model.features.3.0.block.1.1.running_var\", \"vision_model.features.3.1.block.0.0.weight\", \"vision_model.features.3.1.block.0.1.weight\", \"vision_model.features.3.1.block.0.1.bias\", \"vision_model.features.3.1.block.0.1.running_mean\", \"vision_model.features.3.1.block.0.1.running_var\", \"vision_model.features.3.1.block.1.0.weight\", \"vision_model.features.3.1.block.1.1.weight\", \"vision_model.features.3.1.block.1.1.bias\", \"vision_model.features.3.1.block.1.1.running_mean\", \"vision_model.features.3.1.block.1.1.running_var\", \"vision_model.features.3.2.block.0.0.weight\", \"vision_model.features.3.2.block.0.1.weight\", \"vision_model.features.3.2.block.0.1.bias\", \"vision_model.features.3.2.block.0.1.running_mean\", \"vision_model.features.3.2.block.0.1.running_var\", \"vision_model.features.3.2.block.1.0.weight\", \"vision_model.features.3.2.block.1.1.weight\", \"vision_model.features.3.2.block.1.1.bias\", \"vision_model.features.3.2.block.1.1.running_mean\", \"vision_model.features.3.2.block.1.1.running_var\", \"vision_model.features.3.3.block.0.0.weight\", \"vision_model.features.3.3.block.0.1.weight\", \"vision_model.features.3.3.block.0.1.bias\", \"vision_model.features.3.3.block.0.1.running_mean\", \"vision_model.features.3.3.block.0.1.running_var\", \"vision_model.features.3.3.block.1.0.weight\", \"vision_model.features.3.3.block.1.1.weight\", \"vision_model.features.3.3.block.1.1.bias\", \"vision_model.features.3.3.block.1.1.running_mean\", \"vision_model.features.3.3.block.1.1.running_var\", \"vision_model.features.4.0.block.0.0.weight\", \"vision_model.features.4.0.block.0.1.weight\", \"vision_model.features.4.0.block.0.1.bias\", \"vision_model.features.4.0.block.0.1.running_mean\", \"vision_model.features.4.0.block.0.1.running_var\", \"vision_model.features.4.0.block.1.0.weight\", \"vision_model.features.4.0.block.1.1.weight\", \"vision_model.features.4.0.block.1.1.bias\", \"vision_model.features.4.0.block.1.1.running_mean\", \"vision_model.features.4.0.block.1.1.running_var\", \"vision_model.features.4.0.block.2.fc1.weight\", \"vision_model.features.4.0.block.2.fc1.bias\", \"vision_model.features.4.0.block.2.fc2.weight\", \"vision_model.features.4.0.block.2.fc2.bias\", \"vision_model.features.4.0.block.3.0.weight\", \"vision_model.features.4.0.block.3.1.weight\", \"vision_model.features.4.0.block.3.1.bias\", \"vision_model.features.4.0.block.3.1.running_mean\", \"vision_model.features.4.0.block.3.1.running_var\", \"vision_model.features.4.1.block.0.0.weight\", \"vision_model.features.4.1.block.0.1.weight\", \"vision_model.features.4.1.block.0.1.bias\", \"vision_model.features.4.1.block.0.1.running_mean\", \"vision_model.features.4.1.block.0.1.running_var\", \"vision_model.features.4.1.block.1.0.weight\", \"vision_model.features.4.1.block.1.1.weight\", \"vision_model.features.4.1.block.1.1.bias\", \"vision_model.features.4.1.block.1.1.running_mean\", \"vision_model.features.4.1.block.1.1.running_var\", \"vision_model.features.4.1.block.2.fc1.weight\", \"vision_model.features.4.1.block.2.fc1.bias\", \"vision_model.features.4.1.block.2.fc2.weight\", \"vision_model.features.4.1.block.2.fc2.bias\", \"vision_model.features.4.1.block.3.0.weight\", \"vision_model.features.4.1.block.3.1.weight\", \"vision_model.features.4.1.block.3.1.bias\", \"vision_model.features.4.1.block.3.1.running_mean\", \"vision_model.features.4.1.block.3.1.running_var\", \"vision_model.features.4.2.block.0.0.weight\", \"vision_model.features.4.2.block.0.1.weight\", \"vision_model.features.4.2.block.0.1.bias\", \"vision_model.features.4.2.block.0.1.running_mean\", \"vision_model.features.4.2.block.0.1.running_var\", \"vision_model.features.4.2.block.1.0.weight\", \"vision_model.features.4.2.block.1.1.weight\", \"vision_model.features.4.2.block.1.1.bias\", \"vision_model.features.4.2.block.1.1.running_mean\", \"vision_model.features.4.2.block.1.1.running_var\", \"vision_model.features.4.2.block.2.fc1.weight\", \"vision_model.features.4.2.block.2.fc1.bias\", \"vision_model.features.4.2.block.2.fc2.weight\", \"vision_model.features.4.2.block.2.fc2.bias\", \"vision_model.features.4.2.block.3.0.weight\", \"vision_model.features.4.2.block.3.1.weight\", \"vision_model.features.4.2.block.3.1.bias\", \"vision_model.features.4.2.block.3.1.running_mean\", \"vision_model.features.4.2.block.3.1.running_var\", \"vision_model.features.4.3.block.0.0.weight\", \"vision_model.features.4.3.block.0.1.weight\", \"vision_model.features.4.3.block.0.1.bias\", \"vision_model.features.4.3.block.0.1.running_mean\", \"vision_model.features.4.3.block.0.1.running_var\", \"vision_model.features.4.3.block.1.0.weight\", \"vision_model.features.4.3.block.1.1.weight\", \"vision_model.features.4.3.block.1.1.bias\", \"vision_model.features.4.3.block.1.1.running_mean\", \"vision_model.features.4.3.block.1.1.running_var\", \"vision_model.features.4.3.block.2.fc1.weight\", \"vision_model.features.4.3.block.2.fc1.bias\", \"vision_model.features.4.3.block.2.fc2.weight\", \"vision_model.features.4.3.block.2.fc2.bias\", \"vision_model.features.4.3.block.3.0.weight\", \"vision_model.features.4.3.block.3.1.weight\", \"vision_model.features.4.3.block.3.1.bias\", \"vision_model.features.4.3.block.3.1.running_mean\", \"vision_model.features.4.3.block.3.1.running_var\", \"vision_model.features.4.4.block.0.0.weight\", \"vision_model.features.4.4.block.0.1.weight\", \"vision_model.features.4.4.block.0.1.bias\", \"vision_model.features.4.4.block.0.1.running_mean\", \"vision_model.features.4.4.block.0.1.running_var\", \"vision_model.features.4.4.block.1.0.weight\", \"vision_model.features.4.4.block.1.1.weight\", \"vision_model.features.4.4.block.1.1.bias\", \"vision_model.features.4.4.block.1.1.running_mean\", \"vision_model.features.4.4.block.1.1.running_var\", \"vision_model.features.4.4.block.2.fc1.weight\", \"vision_model.features.4.4.block.2.fc1.bias\", \"vision_model.features.4.4.block.2.fc2.weight\", \"vision_model.features.4.4.block.2.fc2.bias\", \"vision_model.features.4.4.block.3.0.weight\", \"vision_model.features.4.4.block.3.1.weight\", \"vision_model.features.4.4.block.3.1.bias\", \"vision_model.features.4.4.block.3.1.running_mean\", \"vision_model.features.4.4.block.3.1.running_var\", \"vision_model.features.4.5.block.0.0.weight\", \"vision_model.features.4.5.block.0.1.weight\", \"vision_model.features.4.5.block.0.1.bias\", \"vision_model.features.4.5.block.0.1.running_mean\", \"vision_model.features.4.5.block.0.1.running_var\", \"vision_model.features.4.5.block.1.0.weight\", \"vision_model.features.4.5.block.1.1.weight\", \"vision_model.features.4.5.block.1.1.bias\", \"vision_model.features.4.5.block.1.1.running_mean\", \"vision_model.features.4.5.block.1.1.running_var\", \"vision_model.features.4.5.block.2.fc1.weight\", \"vision_model.features.4.5.block.2.fc1.bias\", \"vision_model.features.4.5.block.2.fc2.weight\", \"vision_model.features.4.5.block.2.fc2.bias\", \"vision_model.features.4.5.block.3.0.weight\", \"vision_model.features.4.5.block.3.1.weight\", \"vision_model.features.4.5.block.3.1.bias\", \"vision_model.features.4.5.block.3.1.running_mean\", \"vision_model.features.4.5.block.3.1.running_var\", \"vision_model.features.5.0.block.0.0.weight\", \"vision_model.features.5.0.block.0.1.weight\", \"vision_model.features.5.0.block.0.1.bias\", \"vision_model.features.5.0.block.0.1.running_mean\", \"vision_model.features.5.0.block.0.1.running_var\", \"vision_model.features.5.0.block.1.0.weight\", \"vision_model.features.5.0.block.1.1.weight\", \"vision_model.features.5.0.block.1.1.bias\", \"vision_model.features.5.0.block.1.1.running_mean\", \"vision_model.features.5.0.block.1.1.running_var\", \"vision_model.features.5.0.block.2.fc1.weight\", \"vision_model.features.5.0.block.2.fc1.bias\", \"vision_model.features.5.0.block.2.fc2.weight\", \"vision_model.features.5.0.block.2.fc2.bias\", \"vision_model.features.5.0.block.3.0.weight\", \"vision_model.features.5.0.block.3.1.weight\", \"vision_model.features.5.0.block.3.1.bias\", \"vision_model.features.5.0.block.3.1.running_mean\", \"vision_model.features.5.0.block.3.1.running_var\", \"vision_model.features.5.1.block.0.0.weight\", \"vision_model.features.5.1.block.0.1.weight\", \"vision_model.features.5.1.block.0.1.bias\", \"vision_model.features.5.1.block.0.1.running_mean\", \"vision_model.features.5.1.block.0.1.running_var\", \"vision_model.features.5.1.block.1.0.weight\", \"vision_model.features.5.1.block.1.1.weight\", \"vision_model.features.5.1.block.1.1.bias\", \"vision_model.features.5.1.block.1.1.running_mean\", \"vision_model.features.5.1.block.1.1.running_var\", \"vision_model.features.5.1.block.2.fc1.weight\", \"vision_model.features.5.1.block.2.fc1.bias\", \"vision_model.features.5.1.block.2.fc2.weight\", \"vision_model.features.5.1.block.2.fc2.bias\", \"vision_model.features.5.1.block.3.0.weight\", \"vision_model.features.5.1.block.3.1.weight\", \"vision_model.features.5.1.block.3.1.bias\", \"vision_model.features.5.1.block.3.1.running_mean\", \"vision_model.features.5.1.block.3.1.running_var\", \"vision_model.features.5.2.block.0.0.weight\", \"vision_model.features.5.2.block.0.1.weight\", \"vision_model.features.5.2.block.0.1.bias\", \"vision_model.features.5.2.block.0.1.running_mean\", \"vision_model.features.5.2.block.0.1.running_var\", \"vision_model.features.5.2.block.1.0.weight\", \"vision_model.features.5.2.block.1.1.weight\", \"vision_model.features.5.2.block.1.1.bias\", \"vision_model.features.5.2.block.1.1.running_mean\", \"vision_model.features.5.2.block.1.1.running_var\", \"vision_model.features.5.2.block.2.fc1.weight\", \"vision_model.features.5.2.block.2.fc1.bias\", \"vision_model.features.5.2.block.2.fc2.weight\", \"vision_model.features.5.2.block.2.fc2.bias\", \"vision_model.features.5.2.block.3.0.weight\", \"vision_model.features.5.2.block.3.1.weight\", \"vision_model.features.5.2.block.3.1.bias\", \"vision_model.features.5.2.block.3.1.running_mean\", \"vision_model.features.5.2.block.3.1.running_var\", \"vision_model.features.5.3.block.0.0.weight\", \"vision_model.features.5.3.block.0.1.weight\", \"vision_model.features.5.3.block.0.1.bias\", \"vision_model.features.5.3.block.0.1.running_mean\", \"vision_model.features.5.3.block.0.1.running_var\", \"vision_model.features.5.3.block.1.0.weight\", \"vision_model.features.5.3.block.1.1.weight\", \"vision_model.features.5.3.block.1.1.bias\", \"vision_model.features.5.3.block.1.1.running_mean\", \"vision_model.features.5.3.block.1.1.running_var\", \"vision_model.features.5.3.block.2.fc1.weight\", \"vision_model.features.5.3.block.2.fc1.bias\", \"vision_model.features.5.3.block.2.fc2.weight\", \"vision_model.features.5.3.block.2.fc2.bias\", \"vision_model.features.5.3.block.3.0.weight\", \"vision_model.features.5.3.block.3.1.weight\", \"vision_model.features.5.3.block.3.1.bias\", \"vision_model.features.5.3.block.3.1.running_mean\", \"vision_model.features.5.3.block.3.1.running_var\", \"vision_model.features.5.4.block.0.0.weight\", \"vision_model.features.5.4.block.0.1.weight\", \"vision_model.features.5.4.block.0.1.bias\", \"vision_model.features.5.4.block.0.1.running_mean\", \"vision_model.features.5.4.block.0.1.running_var\", \"vision_model.features.5.4.block.1.0.weight\", \"vision_model.features.5.4.block.1.1.weight\", \"vision_model.features.5.4.block.1.1.bias\", \"vision_model.features.5.4.block.1.1.running_mean\", \"vision_model.features.5.4.block.1.1.running_var\", \"vision_model.features.5.4.block.2.fc1.weight\", \"vision_model.features.5.4.block.2.fc1.bias\", \"vision_model.features.5.4.block.2.fc2.weight\", \"vision_model.features.5.4.block.2.fc2.bias\", \"vision_model.features.5.4.block.3.0.weight\", \"vision_model.features.5.4.block.3.1.weight\", \"vision_model.features.5.4.block.3.1.bias\", \"vision_model.features.5.4.block.3.1.running_mean\", \"vision_model.features.5.4.block.3.1.running_var\", \"vision_model.features.5.5.block.0.0.weight\", \"vision_model.features.5.5.block.0.1.weight\", \"vision_model.features.5.5.block.0.1.bias\", \"vision_model.features.5.5.block.0.1.running_mean\", \"vision_model.features.5.5.block.0.1.running_var\", \"vision_model.features.5.5.block.1.0.weight\", \"vision_model.features.5.5.block.1.1.weight\", \"vision_model.features.5.5.block.1.1.bias\", \"vision_model.features.5.5.block.1.1.running_mean\", \"vision_model.features.5.5.block.1.1.running_var\", \"vision_model.features.5.5.block.2.fc1.weight\", \"vision_model.features.5.5.block.2.fc1.bias\", \"vision_model.features.5.5.block.2.fc2.weight\", \"vision_model.features.5.5.block.2.fc2.bias\", \"vision_model.features.5.5.block.3.0.weight\", \"vision_model.features.5.5.block.3.1.weight\", \"vision_model.features.5.5.block.3.1.bias\", \"vision_model.features.5.5.block.3.1.running_mean\", \"vision_model.features.5.5.block.3.1.running_var\", \"vision_model.features.5.6.block.0.0.weight\", \"vision_model.features.5.6.block.0.1.weight\", \"vision_model.features.5.6.block.0.1.bias\", \"vision_model.features.5.6.block.0.1.running_mean\", \"vision_model.features.5.6.block.0.1.running_var\", \"vision_model.features.5.6.block.1.0.weight\", \"vision_model.features.5.6.block.1.1.weight\", \"vision_model.features.5.6.block.1.1.bias\", \"vision_model.features.5.6.block.1.1.running_mean\", \"vision_model.features.5.6.block.1.1.running_var\", \"vision_model.features.5.6.block.2.fc1.weight\", \"vision_model.features.5.6.block.2.fc1.bias\", \"vision_model.features.5.6.block.2.fc2.weight\", \"vision_model.features.5.6.block.2.fc2.bias\", \"vision_model.features.5.6.block.3.0.weight\", \"vision_model.features.5.6.block.3.1.weight\", \"vision_model.features.5.6.block.3.1.bias\", \"vision_model.features.5.6.block.3.1.running_mean\", \"vision_model.features.5.6.block.3.1.running_var\", \"vision_model.features.5.7.block.0.0.weight\", \"vision_model.features.5.7.block.0.1.weight\", \"vision_model.features.5.7.block.0.1.bias\", \"vision_model.features.5.7.block.0.1.running_mean\", \"vision_model.features.5.7.block.0.1.running_var\", \"vision_model.features.5.7.block.1.0.weight\", \"vision_model.features.5.7.block.1.1.weight\", \"vision_model.features.5.7.block.1.1.bias\", \"vision_model.features.5.7.block.1.1.running_mean\", \"vision_model.features.5.7.block.1.1.running_var\", \"vision_model.features.5.7.block.2.fc1.weight\", \"vision_model.features.5.7.block.2.fc1.bias\", \"vision_model.features.5.7.block.2.fc2.weight\", \"vision_model.features.5.7.block.2.fc2.bias\", \"vision_model.features.5.7.block.3.0.weight\", \"vision_model.features.5.7.block.3.1.weight\", \"vision_model.features.5.7.block.3.1.bias\", \"vision_model.features.5.7.block.3.1.running_mean\", \"vision_model.features.5.7.block.3.1.running_var\", \"vision_model.features.5.8.block.0.0.weight\", \"vision_model.features.5.8.block.0.1.weight\", \"vision_model.features.5.8.block.0.1.bias\", \"vision_model.features.5.8.block.0.1.running_mean\", \"vision_model.features.5.8.block.0.1.running_var\", \"vision_model.features.5.8.block.1.0.weight\", \"vision_model.features.5.8.block.1.1.weight\", \"vision_model.features.5.8.block.1.1.bias\", \"vision_model.features.5.8.block.1.1.running_mean\", \"vision_model.features.5.8.block.1.1.running_var\", \"vision_model.features.5.8.block.2.fc1.weight\", \"vision_model.features.5.8.block.2.fc1.bias\", \"vision_model.features.5.8.block.2.fc2.weight\", \"vision_model.features.5.8.block.2.fc2.bias\", \"vision_model.features.5.8.block.3.0.weight\", \"vision_model.features.5.8.block.3.1.weight\", \"vision_model.features.5.8.block.3.1.bias\", \"vision_model.features.5.8.block.3.1.running_mean\", \"vision_model.features.5.8.block.3.1.running_var\", \"vision_model.features.6.0.block.0.0.weight\", \"vision_model.features.6.0.block.0.1.weight\", \"vision_model.features.6.0.block.0.1.bias\", \"vision_model.features.6.0.block.0.1.running_mean\", \"vision_model.features.6.0.block.0.1.running_var\", \"vision_model.features.6.0.block.1.0.weight\", \"vision_model.features.6.0.block.1.1.weight\", \"vision_model.features.6.0.block.1.1.bias\", \"vision_model.features.6.0.block.1.1.running_mean\", \"vision_model.features.6.0.block.1.1.running_var\", \"vision_model.features.6.0.block.2.fc1.weight\", \"vision_model.features.6.0.block.2.fc1.bias\", \"vision_model.features.6.0.block.2.fc2.weight\", \"vision_model.features.6.0.block.2.fc2.bias\", \"vision_model.features.6.0.block.3.0.weight\", \"vision_model.features.6.0.block.3.1.weight\", \"vision_model.features.6.0.block.3.1.bias\", \"vision_model.features.6.0.block.3.1.running_mean\", \"vision_model.features.6.0.block.3.1.running_var\", \"vision_model.features.6.1.block.0.0.weight\", \"vision_model.features.6.1.block.0.1.weight\", \"vision_model.features.6.1.block.0.1.bias\", \"vision_model.features.6.1.block.0.1.running_mean\", \"vision_model.features.6.1.block.0.1.running_var\", \"vision_model.features.6.1.block.1.0.weight\", \"vision_model.features.6.1.block.1.1.weight\", \"vision_model.features.6.1.block.1.1.bias\", \"vision_model.features.6.1.block.1.1.running_mean\", \"vision_model.features.6.1.block.1.1.running_var\", \"vision_model.features.6.1.block.2.fc1.weight\", \"vision_model.features.6.1.block.2.fc1.bias\", \"vision_model.features.6.1.block.2.fc2.weight\", \"vision_model.features.6.1.block.2.fc2.bias\", \"vision_model.features.6.1.block.3.0.weight\", \"vision_model.features.6.1.block.3.1.weight\", \"vision_model.features.6.1.block.3.1.bias\", \"vision_model.features.6.1.block.3.1.running_mean\", \"vision_model.features.6.1.block.3.1.running_var\", \"vision_model.features.6.2.block.0.0.weight\", \"vision_model.features.6.2.block.0.1.weight\", \"vision_model.features.6.2.block.0.1.bias\", \"vision_model.features.6.2.block.0.1.running_mean\", \"vision_model.features.6.2.block.0.1.running_var\", \"vision_model.features.6.2.block.1.0.weight\", \"vision_model.features.6.2.block.1.1.weight\", \"vision_model.features.6.2.block.1.1.bias\", \"vision_model.features.6.2.block.1.1.running_mean\", \"vision_model.features.6.2.block.1.1.running_var\", \"vision_model.features.6.2.block.2.fc1.weight\", \"vision_model.features.6.2.block.2.fc1.bias\", \"vision_model.features.6.2.block.2.fc2.weight\", \"vision_model.features.6.2.block.2.fc2.bias\", \"vision_model.features.6.2.block.3.0.weight\", \"vision_model.features.6.2.block.3.1.weight\", \"vision_model.features.6.2.block.3.1.bias\", \"vision_model.features.6.2.block.3.1.running_mean\", \"vision_model.features.6.2.block.3.1.running_var\", \"vision_model.features.6.3.block.0.0.weight\", \"vision_model.features.6.3.block.0.1.weight\", \"vision_model.features.6.3.block.0.1.bias\", \"vision_model.features.6.3.block.0.1.running_mean\", \"vision_model.features.6.3.block.0.1.running_var\", \"vision_model.features.6.3.block.1.0.weight\", \"vision_model.features.6.3.block.1.1.weight\", \"vision_model.features.6.3.block.1.1.bias\", \"vision_model.features.6.3.block.1.1.running_mean\", \"vision_model.features.6.3.block.1.1.running_var\", \"vision_model.features.6.3.block.2.fc1.weight\", \"vision_model.features.6.3.block.2.fc1.bias\", \"vision_model.features.6.3.block.2.fc2.weight\", \"vision_model.features.6.3.block.2.fc2.bias\", \"vision_model.features.6.3.block.3.0.weight\", \"vision_model.features.6.3.block.3.1.weight\", \"vision_model.features.6.3.block.3.1.bias\", \"vision_model.features.6.3.block.3.1.running_mean\", \"vision_model.features.6.3.block.3.1.running_var\", \"vision_model.features.6.4.block.0.0.weight\", \"vision_model.features.6.4.block.0.1.weight\", \"vision_model.features.6.4.block.0.1.bias\", \"vision_model.features.6.4.block.0.1.running_mean\", \"vision_model.features.6.4.block.0.1.running_var\", \"vision_model.features.6.4.block.1.0.weight\", \"vision_model.features.6.4.block.1.1.weight\", \"vision_model.features.6.4.block.1.1.bias\", \"vision_model.features.6.4.block.1.1.running_mean\", \"vision_model.features.6.4.block.1.1.running_var\", \"vision_model.features.6.4.block.2.fc1.weight\", \"vision_model.features.6.4.block.2.fc1.bias\", \"vision_model.features.6.4.block.2.fc2.weight\", \"vision_model.features.6.4.block.2.fc2.bias\", \"vision_model.features.6.4.block.3.0.weight\", \"vision_model.features.6.4.block.3.1.weight\", \"vision_model.features.6.4.block.3.1.bias\", \"vision_model.features.6.4.block.3.1.running_mean\", \"vision_model.features.6.4.block.3.1.running_var\", \"vision_model.features.6.5.block.0.0.weight\", \"vision_model.features.6.5.block.0.1.weight\", \"vision_model.features.6.5.block.0.1.bias\", \"vision_model.features.6.5.block.0.1.running_mean\", \"vision_model.features.6.5.block.0.1.running_var\", \"vision_model.features.6.5.block.1.0.weight\", \"vision_model.features.6.5.block.1.1.weight\", \"vision_model.features.6.5.block.1.1.bias\", \"vision_model.features.6.5.block.1.1.running_mean\", \"vision_model.features.6.5.block.1.1.running_var\", \"vision_model.features.6.5.block.2.fc1.weight\", \"vision_model.features.6.5.block.2.fc1.bias\", \"vision_model.features.6.5.block.2.fc2.weight\", \"vision_model.features.6.5.block.2.fc2.bias\", \"vision_model.features.6.5.block.3.0.weight\", \"vision_model.features.6.5.block.3.1.weight\", \"vision_model.features.6.5.block.3.1.bias\", \"vision_model.features.6.5.block.3.1.running_mean\", \"vision_model.features.6.5.block.3.1.running_var\", \"vision_model.features.6.6.block.0.0.weight\", \"vision_model.features.6.6.block.0.1.weight\", \"vision_model.features.6.6.block.0.1.bias\", \"vision_model.features.6.6.block.0.1.running_mean\", \"vision_model.features.6.6.block.0.1.running_var\", \"vision_model.features.6.6.block.1.0.weight\", \"vision_model.features.6.6.block.1.1.weight\", \"vision_model.features.6.6.block.1.1.bias\", \"vision_model.features.6.6.block.1.1.running_mean\", \"vision_model.features.6.6.block.1.1.running_var\", \"vision_model.features.6.6.block.2.fc1.weight\", \"vision_model.features.6.6.block.2.fc1.bias\", \"vision_model.features.6.6.block.2.fc2.weight\", \"vision_model.features.6.6.block.2.fc2.bias\", \"vision_model.features.6.6.block.3.0.weight\", \"vision_model.features.6.6.block.3.1.weight\", \"vision_model.features.6.6.block.3.1.bias\", \"vision_model.features.6.6.block.3.1.running_mean\", \"vision_model.features.6.6.block.3.1.running_var\", \"vision_model.features.6.7.block.0.0.weight\", \"vision_model.features.6.7.block.0.1.weight\", \"vision_model.features.6.7.block.0.1.bias\", \"vision_model.features.6.7.block.0.1.running_mean\", \"vision_model.features.6.7.block.0.1.running_var\", \"vision_model.features.6.7.block.1.0.weight\", \"vision_model.features.6.7.block.1.1.weight\", \"vision_model.features.6.7.block.1.1.bias\", \"vision_model.features.6.7.block.1.1.running_mean\", \"vision_model.features.6.7.block.1.1.running_var\", \"vision_model.features.6.7.block.2.fc1.weight\", \"vision_model.features.6.7.block.2.fc1.bias\", \"vision_model.features.6.7.block.2.fc2.weight\", \"vision_model.features.6.7.block.2.fc2.bias\", \"vision_model.features.6.7.block.3.0.weight\", \"vision_model.features.6.7.block.3.1.weight\", \"vision_model.features.6.7.block.3.1.bias\", \"vision_model.features.6.7.block.3.1.running_mean\", \"vision_model.features.6.7.block.3.1.running_var\", \"vision_model.features.6.8.block.0.0.weight\", \"vision_model.features.6.8.block.0.1.weight\", \"vision_model.features.6.8.block.0.1.bias\", \"vision_model.features.6.8.block.0.1.running_mean\", \"vision_model.features.6.8.block.0.1.running_var\", \"vision_model.features.6.8.block.1.0.weight\", \"vision_model.features.6.8.block.1.1.weight\", \"vision_model.features.6.8.block.1.1.bias\", \"vision_model.features.6.8.block.1.1.running_mean\", \"vision_model.features.6.8.block.1.1.running_var\", \"vision_model.features.6.8.block.2.fc1.weight\", \"vision_model.features.6.8.block.2.fc1.bias\", \"vision_model.features.6.8.block.2.fc2.weight\", \"vision_model.features.6.8.block.2.fc2.bias\", \"vision_model.features.6.8.block.3.0.weight\", \"vision_model.features.6.8.block.3.1.weight\", \"vision_model.features.6.8.block.3.1.bias\", \"vision_model.features.6.8.block.3.1.running_mean\", \"vision_model.features.6.8.block.3.1.running_var\", \"vision_model.features.6.9.block.0.0.weight\", \"vision_model.features.6.9.block.0.1.weight\", \"vision_model.features.6.9.block.0.1.bias\", \"vision_model.features.6.9.block.0.1.running_mean\", \"vision_model.features.6.9.block.0.1.running_var\", \"vision_model.features.6.9.block.1.0.weight\", \"vision_model.features.6.9.block.1.1.weight\", \"vision_model.features.6.9.block.1.1.bias\", \"vision_model.features.6.9.block.1.1.running_mean\", \"vision_model.features.6.9.block.1.1.running_var\", \"vision_model.features.6.9.block.2.fc1.weight\", \"vision_model.features.6.9.block.2.fc1.bias\", \"vision_model.features.6.9.block.2.fc2.weight\", \"vision_model.features.6.9.block.2.fc2.bias\", \"vision_model.features.6.9.block.3.0.weight\", \"vision_model.features.6.9.block.3.1.weight\", \"vision_model.features.6.9.block.3.1.bias\", \"vision_model.features.6.9.block.3.1.running_mean\", \"vision_model.features.6.9.block.3.1.running_var\", \"vision_model.features.6.10.block.0.0.weight\", \"vision_model.features.6.10.block.0.1.weight\", \"vision_model.features.6.10.block.0.1.bias\", \"vision_model.features.6.10.block.0.1.running_mean\", \"vision_model.features.6.10.block.0.1.running_var\", \"vision_model.features.6.10.block.1.0.weight\", \"vision_model.features.6.10.block.1.1.weight\", \"vision_model.features.6.10.block.1.1.bias\", \"vision_model.features.6.10.block.1.1.running_mean\", \"vision_model.features.6.10.block.1.1.running_var\", \"vision_model.features.6.10.block.2.fc1.weight\", \"vision_model.features.6.10.block.2.fc1.bias\", \"vision_model.features.6.10.block.2.fc2.weight\", \"vision_model.features.6.10.block.2.fc2.bias\", \"vision_model.features.6.10.block.3.0.weight\", \"vision_model.features.6.10.block.3.1.weight\", \"vision_model.features.6.10.block.3.1.bias\", \"vision_model.features.6.10.block.3.1.running_mean\", \"vision_model.features.6.10.block.3.1.running_var\", \"vision_model.features.6.11.block.0.0.weight\", \"vision_model.features.6.11.block.0.1.weight\", \"vision_model.features.6.11.block.0.1.bias\", \"vision_model.features.6.11.block.0.1.running_mean\", \"vision_model.features.6.11.block.0.1.running_var\", \"vision_model.features.6.11.block.1.0.weight\", \"vision_model.features.6.11.block.1.1.weight\", \"vision_model.features.6.11.block.1.1.bias\", \"vision_model.features.6.11.block.1.1.running_mean\", \"vision_model.features.6.11.block.1.1.running_var\", \"vision_model.features.6.11.block.2.fc1.weight\", \"vision_model.features.6.11.block.2.fc1.bias\", \"vision_model.features.6.11.block.2.fc2.weight\", \"vision_model.features.6.11.block.2.fc2.bias\", \"vision_model.features.6.11.block.3.0.weight\", \"vision_model.features.6.11.block.3.1.weight\", \"vision_model.features.6.11.block.3.1.bias\", \"vision_model.features.6.11.block.3.1.running_mean\", \"vision_model.features.6.11.block.3.1.running_var\", \"vision_model.features.6.12.block.0.0.weight\", \"vision_model.features.6.12.block.0.1.weight\", \"vision_model.features.6.12.block.0.1.bias\", \"vision_model.features.6.12.block.0.1.running_mean\", \"vision_model.features.6.12.block.0.1.running_var\", \"vision_model.features.6.12.block.1.0.weight\", \"vision_model.features.6.12.block.1.1.weight\", \"vision_model.features.6.12.block.1.1.bias\", \"vision_model.features.6.12.block.1.1.running_mean\", \"vision_model.features.6.12.block.1.1.running_var\", \"vision_model.features.6.12.block.2.fc1.weight\", \"vision_model.features.6.12.block.2.fc1.bias\", \"vision_model.features.6.12.block.2.fc2.weight\", \"vision_model.features.6.12.block.2.fc2.bias\", \"vision_model.features.6.12.block.3.0.weight\", \"vision_model.features.6.12.block.3.1.weight\", \"vision_model.features.6.12.block.3.1.bias\", \"vision_model.features.6.12.block.3.1.running_mean\", \"vision_model.features.6.12.block.3.1.running_var\", \"vision_model.features.6.13.block.0.0.weight\", \"vision_model.features.6.13.block.0.1.weight\", \"vision_model.features.6.13.block.0.1.bias\", \"vision_model.features.6.13.block.0.1.running_mean\", \"vision_model.features.6.13.block.0.1.running_var\", \"vision_model.features.6.13.block.1.0.weight\", \"vision_model.features.6.13.block.1.1.weight\", \"vision_model.features.6.13.block.1.1.bias\", \"vision_model.features.6.13.block.1.1.running_mean\", \"vision_model.features.6.13.block.1.1.running_var\", \"vision_model.features.6.13.block.2.fc1.weight\", \"vision_model.features.6.13.block.2.fc1.bias\", \"vision_model.features.6.13.block.2.fc2.weight\", \"vision_model.features.6.13.block.2.fc2.bias\", \"vision_model.features.6.13.block.3.0.weight\", \"vision_model.features.6.13.block.3.1.weight\", \"vision_model.features.6.13.block.3.1.bias\", \"vision_model.features.6.13.block.3.1.running_mean\", \"vision_model.features.6.13.block.3.1.running_var\", \"vision_model.features.6.14.block.0.0.weight\", \"vision_model.features.6.14.block.0.1.weight\", \"vision_model.features.6.14.block.0.1.bias\", \"vision_model.features.6.14.block.0.1.running_mean\", \"vision_model.features.6.14.block.0.1.running_var\", \"vision_model.features.6.14.block.1.0.weight\", \"vision_model.features.6.14.block.1.1.weight\", \"vision_model.features.6.14.block.1.1.bias\", \"vision_model.features.6.14.block.1.1.running_mean\", \"vision_model.features.6.14.block.1.1.running_var\", \"vision_model.features.6.14.block.2.fc1.weight\", \"vision_model.features.6.14.block.2.fc1.bias\", \"vision_model.features.6.14.block.2.fc2.weight\", \"vision_model.features.6.14.block.2.fc2.bias\", \"vision_model.features.6.14.block.3.0.weight\", \"vision_model.features.6.14.block.3.1.weight\", \"vision_model.features.6.14.block.3.1.bias\", \"vision_model.features.6.14.block.3.1.running_mean\", \"vision_model.features.6.14.block.3.1.running_var\", \"vision_model.features.7.0.weight\", \"vision_model.features.7.1.weight\", \"vision_model.features.7.1.bias\", \"vision_model.features.7.1.running_mean\", \"vision_model.features.7.1.running_var\", \"text_model.embeddings.word_embeddings.weight\", \"text_model.embeddings.LayerNorm.weight\", \"text_model.embeddings.LayerNorm.bias\", \"text_model.encoder.layer.0.attention.self.q_bias\", \"text_model.encoder.layer.0.attention.self.v_bias\", \"text_model.encoder.layer.0.attention.self.in_proj.weight\", \"text_model.encoder.layer.0.attention.self.pos_proj.weight\", \"text_model.encoder.layer.0.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.0.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.0.attention.output.dense.weight\", \"text_model.encoder.layer.0.attention.output.dense.bias\", \"text_model.encoder.layer.0.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.0.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.0.intermediate.dense.weight\", \"text_model.encoder.layer.0.intermediate.dense.bias\", \"text_model.encoder.layer.0.output.dense.weight\", \"text_model.encoder.layer.0.output.dense.bias\", \"text_model.encoder.layer.0.output.LayerNorm.weight\", \"text_model.encoder.layer.0.output.LayerNorm.bias\", \"text_model.encoder.layer.1.attention.self.q_bias\", \"text_model.encoder.layer.1.attention.self.v_bias\", \"text_model.encoder.layer.1.attention.self.in_proj.weight\", \"text_model.encoder.layer.1.attention.self.pos_proj.weight\", \"text_model.encoder.layer.1.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.1.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.1.attention.output.dense.weight\", \"text_model.encoder.layer.1.attention.output.dense.bias\", \"text_model.encoder.layer.1.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.1.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.1.intermediate.dense.weight\", \"text_model.encoder.layer.1.intermediate.dense.bias\", \"text_model.encoder.layer.1.output.dense.weight\", \"text_model.encoder.layer.1.output.dense.bias\", \"text_model.encoder.layer.1.output.LayerNorm.weight\", \"text_model.encoder.layer.1.output.LayerNorm.bias\", \"text_model.encoder.layer.2.attention.self.q_bias\", \"text_model.encoder.layer.2.attention.self.v_bias\", \"text_model.encoder.layer.2.attention.self.in_proj.weight\", \"text_model.encoder.layer.2.attention.self.pos_proj.weight\", \"text_model.encoder.layer.2.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.2.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.2.attention.output.dense.weight\", \"text_model.encoder.layer.2.attention.output.dense.bias\", \"text_model.encoder.layer.2.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.2.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.2.intermediate.dense.weight\", \"text_model.encoder.layer.2.intermediate.dense.bias\", \"text_model.encoder.layer.2.output.dense.weight\", \"text_model.encoder.layer.2.output.dense.bias\", \"text_model.encoder.layer.2.output.LayerNorm.weight\", \"text_model.encoder.layer.2.output.LayerNorm.bias\", \"text_model.encoder.layer.3.attention.self.q_bias\", \"text_model.encoder.layer.3.attention.self.v_bias\", \"text_model.encoder.layer.3.attention.self.in_proj.weight\", \"text_model.encoder.layer.3.attention.self.pos_proj.weight\", \"text_model.encoder.layer.3.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.3.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.3.attention.output.dense.weight\", \"text_model.encoder.layer.3.attention.output.dense.bias\", \"text_model.encoder.layer.3.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.3.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.3.intermediate.dense.weight\", \"text_model.encoder.layer.3.intermediate.dense.bias\", \"text_model.encoder.layer.3.output.dense.weight\", \"text_model.encoder.layer.3.output.dense.bias\", \"text_model.encoder.layer.3.output.LayerNorm.weight\", \"text_model.encoder.layer.3.output.LayerNorm.bias\", \"text_model.encoder.layer.4.attention.self.q_bias\", \"text_model.encoder.layer.4.attention.self.v_bias\", \"text_model.encoder.layer.4.attention.self.in_proj.weight\", \"text_model.encoder.layer.4.attention.self.pos_proj.weight\", \"text_model.encoder.layer.4.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.4.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.4.attention.output.dense.weight\", \"text_model.encoder.layer.4.attention.output.dense.bias\", \"text_model.encoder.layer.4.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.4.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.4.intermediate.dense.weight\", \"text_model.encoder.layer.4.intermediate.dense.bias\", \"text_model.encoder.layer.4.output.dense.weight\", \"text_model.encoder.layer.4.output.dense.bias\", \"text_model.encoder.layer.4.output.LayerNorm.weight\", \"text_model.encoder.layer.4.output.LayerNorm.bias\", \"text_model.encoder.layer.5.attention.self.q_bias\", \"text_model.encoder.layer.5.attention.self.v_bias\", \"text_model.encoder.layer.5.attention.self.in_proj.weight\", \"text_model.encoder.layer.5.attention.self.pos_proj.weight\", \"text_model.encoder.layer.5.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.5.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.5.attention.output.dense.weight\", \"text_model.encoder.layer.5.attention.output.dense.bias\", \"text_model.encoder.layer.5.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.5.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.5.intermediate.dense.weight\", \"text_model.encoder.layer.5.intermediate.dense.bias\", \"text_model.encoder.layer.5.output.dense.weight\", \"text_model.encoder.layer.5.output.dense.bias\", \"text_model.encoder.layer.5.output.LayerNorm.weight\", \"text_model.encoder.layer.5.output.LayerNorm.bias\", \"text_model.encoder.layer.6.attention.self.q_bias\", \"text_model.encoder.layer.6.attention.self.v_bias\", \"text_model.encoder.layer.6.attention.self.in_proj.weight\", \"text_model.encoder.layer.6.attention.self.pos_proj.weight\", \"text_model.encoder.layer.6.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.6.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.6.attention.output.dense.weight\", \"text_model.encoder.layer.6.attention.output.dense.bias\", \"text_model.encoder.layer.6.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.6.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.6.intermediate.dense.weight\", \"text_model.encoder.layer.6.intermediate.dense.bias\", \"text_model.encoder.layer.6.output.dense.weight\", \"text_model.encoder.layer.6.output.dense.bias\", \"text_model.encoder.layer.6.output.LayerNorm.weight\", \"text_model.encoder.layer.6.output.LayerNorm.bias\", \"text_model.encoder.layer.7.attention.self.q_bias\", \"text_model.encoder.layer.7.attention.self.v_bias\", \"text_model.encoder.layer.7.attention.self.in_proj.weight\", \"text_model.encoder.layer.7.attention.self.pos_proj.weight\", \"text_model.encoder.layer.7.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.7.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.7.attention.output.dense.weight\", \"text_model.encoder.layer.7.attention.output.dense.bias\", \"text_model.encoder.layer.7.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.7.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.7.intermediate.dense.weight\", \"text_model.encoder.layer.7.intermediate.dense.bias\", \"text_model.encoder.layer.7.output.dense.weight\", \"text_model.encoder.layer.7.output.dense.bias\", \"text_model.encoder.layer.7.output.LayerNorm.weight\", \"text_model.encoder.layer.7.output.LayerNorm.bias\", \"text_model.encoder.layer.8.attention.self.q_bias\", \"text_model.encoder.layer.8.attention.self.v_bias\", \"text_model.encoder.layer.8.attention.self.in_proj.weight\", \"text_model.encoder.layer.8.attention.self.pos_proj.weight\", \"text_model.encoder.layer.8.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.8.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.8.attention.output.dense.weight\", \"text_model.encoder.layer.8.attention.output.dense.bias\", \"text_model.encoder.layer.8.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.8.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.8.intermediate.dense.weight\", \"text_model.encoder.layer.8.intermediate.dense.bias\", \"text_model.encoder.layer.8.output.dense.weight\", \"text_model.encoder.layer.8.output.dense.bias\", \"text_model.encoder.layer.8.output.LayerNorm.weight\", \"text_model.encoder.layer.8.output.LayerNorm.bias\", \"text_model.encoder.layer.9.attention.self.q_bias\", \"text_model.encoder.layer.9.attention.self.v_bias\", \"text_model.encoder.layer.9.attention.self.in_proj.weight\", \"text_model.encoder.layer.9.attention.self.pos_proj.weight\", \"text_model.encoder.layer.9.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.9.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.9.attention.output.dense.weight\", \"text_model.encoder.layer.9.attention.output.dense.bias\", \"text_model.encoder.layer.9.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.9.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.9.intermediate.dense.weight\", \"text_model.encoder.layer.9.intermediate.dense.bias\", \"text_model.encoder.layer.9.output.dense.weight\", \"text_model.encoder.layer.9.output.dense.bias\", \"text_model.encoder.layer.9.output.LayerNorm.weight\", \"text_model.encoder.layer.9.output.LayerNorm.bias\", \"text_model.encoder.layer.10.attention.self.q_bias\", \"text_model.encoder.layer.10.attention.self.v_bias\", \"text_model.encoder.layer.10.attention.self.in_proj.weight\", \"text_model.encoder.layer.10.attention.self.pos_proj.weight\", \"text_model.encoder.layer.10.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.10.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.10.attention.output.dense.weight\", \"text_model.encoder.layer.10.attention.output.dense.bias\", \"text_model.encoder.layer.10.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.10.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.10.intermediate.dense.weight\", \"text_model.encoder.layer.10.intermediate.dense.bias\", \"text_model.encoder.layer.10.output.dense.weight\", \"text_model.encoder.layer.10.output.dense.bias\", \"text_model.encoder.layer.10.output.LayerNorm.weight\", \"text_model.encoder.layer.10.output.LayerNorm.bias\", \"text_model.encoder.layer.11.attention.self.q_bias\", \"text_model.encoder.layer.11.attention.self.v_bias\", \"text_model.encoder.layer.11.attention.self.in_proj.weight\", \"text_model.encoder.layer.11.attention.self.pos_proj.weight\", \"text_model.encoder.layer.11.attention.self.pos_q_proj.weight\", \"text_model.encoder.layer.11.attention.self.pos_q_proj.bias\", \"text_model.encoder.layer.11.attention.output.dense.weight\", \"text_model.encoder.layer.11.attention.output.dense.bias\", \"text_model.encoder.layer.11.attention.output.LayerNorm.weight\", \"text_model.encoder.layer.11.attention.output.LayerNorm.bias\", \"text_model.encoder.layer.11.intermediate.dense.weight\", \"text_model.encoder.layer.11.intermediate.dense.bias\", \"text_model.encoder.layer.11.output.dense.weight\", \"text_model.encoder.layer.11.output.dense.bias\", \"text_model.encoder.layer.11.output.LayerNorm.weight\", \"text_model.encoder.layer.11.output.LayerNorm.bias\", \"text_model.encoder.rel_embeddings.weight\", \"cross_attention.image_proj.weight\", \"cross_attention.image_proj.bias\", \"cross_attention.text_proj.weight\", \"cross_attention.text_proj.bias\", \"cross_attention.attention.in_proj_weight\", \"cross_attention.attention.in_proj_bias\", \"cross_attention.attention.out_proj.weight\", \"cross_attention.attention.out_proj.bias\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.3.weight\", \"classifier.3.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"model_state_dict\", \"optimizer_state_dict\", \"scheduler_state_dict\", \"loss\". ","output_type":"error"}],"execution_count":29},{"cell_type":"code","source":"print(\"Test dataset length:\", len(test_loader.dataset))\nprint(\"Test dataframe length:\", len(test_df))\nprint(test_df['ImageID'].head()) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T22:43:13.579880Z","iopub.execute_input":"2025-01-31T22:43:13.580218Z","iopub.status.idle":"2025-01-31T22:43:13.585842Z","shell.execute_reply.started":"2025-01-31T22:43:13.580174Z","shell.execute_reply":"2025-01-31T22:43:13.584696Z"}},"outputs":[{"name":"stdout","text":"Test dataset length: 10000\nTest dataframe length: 10000\n0    30000.jpg\n1    30001.jpg\n2    30002.jpg\n3    30003.jpg\n4    30004.jpg\nName: ImageID, dtype: object\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"#print(f\"Number of images: {len(image_ids)}\")\npredictions = generate_predictions(model, test_loader, device)\nprint(f\"Number of predictions: {len(predictions)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T22:43:42.626883Z","iopub.execute_input":"2025-01-31T22:43:42.627218Z"}},"outputs":[{"name":"stderr","text":"Generating predictions:  24%|██▎       | 37/157 [01:31<04:43,  2.36s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# **Création du fichier de soumission**","metadata":{}},{"cell_type":"code","source":"def create_submission(predictions, test_df):\n    submission = pd.DataFrame({'ImageID': test_df['ImageID']})\n    \n    labels = predictions.numpy()\n    submission['Labels'] = [' '.join(str(i+1) for i in range(NUM_LABELS) if label[i] == 1) for label in labels]\n    \n    submission.to_csv('submission.csv', index=False)\n    print(\"Submission file created!\")\n\ncreate_submission(predictions, test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T22:10:53.546589Z","iopub.status.idle":"2025-01-31T22:10:53.547031Z","shell.execute_reply.started":"2025-01-31T22:10:53.546724Z","shell.execute_reply":"2025-01-31T22:10:53.546782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df = pd.read_csv('/kaggle/working/submission.csv')\nsubmission_df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T22:10:53.548108Z","iopub.status.idle":"2025-01-31T22:10:53.548680Z","shell.execute_reply.started":"2025-01-31T22:10:53.548277Z","shell.execute_reply":"2025-01-31T22:10:53.548317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T22:10:53.549402Z","iopub.status.idle":"2025-01-31T22:10:53.549890Z","shell.execute_reply.started":"2025-01-31T22:10:53.549559Z","shell.execute_reply":"2025-01-31T22:10:53.549603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}